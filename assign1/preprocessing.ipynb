{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('datasets/training_data.csv')\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = ' '.join(data['text'])\n",
    "# print(corpus[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Customize pipeline\n",
    "# nlp.remove_pipe('tok2vec')\n",
    "# nlp.remove_pipe('tagger')\n",
    "# nlp.remove_pipe('parser')\n",
    "# nlp.remove_pipe('attribute_ruler')\n",
    "# nlp.remove_pipe('lemmatizer')\n",
    "# nlp.remove_pipe('ner')\n",
    "\n",
    "# nlp.enable_pipe('senter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO fix regex to remove instances like '(Reuters) Reuters'\n",
    "# import re\n",
    "# from spacy.tokens import Doc\n",
    "# from spacy.language import Language\n",
    "\n",
    "# @Language.component(\"regex_cleanup\")\n",
    "# def regex_cleanup(doc):\n",
    "#     cleaned_tokens = []\n",
    "#     for token in doc:\n",
    "#         if not re.match(r'((\\w+))', token.text):\n",
    "#             cleaned_tokens.append(token)\n",
    "#     return Doc(doc.vocab, words=[token.text for token in cleaned_tokens])\n",
    "\n",
    "# nlp.add_pipe(\"regex_cleanup\", first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: process all data\n",
    "small_data = data.head(1000)\n",
    "small_data['tokens'] = small_data['text'].apply(nlp)\n",
    "small_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data['tokens_count'] = small_data['tokens'].apply(len)\n",
    "small_data['tokens_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tokens(tokens):\n",
    "    tokens_without_punctuation = [token for token in tokens if not token.is_punct]\n",
    "    tokens_without_space = [token for token in tokens_without_punctuation if not token.is_space]\n",
    "    tokens_without_stopwords = [token for token in tokens_without_space if not token.is_stop]\n",
    "    return tokens_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text(text):\n",
    "    tokens_lemmatized = [token.lemma_ for token in text]\n",
    "    tokens_lower = [token.lower() for token in tokens_lemmatized]\n",
    "    return ' '.join(tokens_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_embeddings(text):\n",
    "    token_embeddings = [token.vector for token in text]\n",
    "    return token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_ner(text):\n",
    "    return [(token, token.pos_, token.ent_iob_, token.ent_type_) for token in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data['tokens_filtered'] = small_data['tokens'].apply(filter_tokens)\n",
    "small_data['text_filtered'] = small_data['tokens_filtered'].apply(filter_text)\n",
    "small_data['text_embeddings'] = small_data['tokens_filtered'].apply(text_embeddings)\n",
    "small_data['text_ner'] = small_data['tokens_filtered'].apply(text_ner)\n",
    "small_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_entities(text_filtered):\n",
    "    entity_dict = {}\n",
    "    doc = nlp(text_filtered)\n",
    "    for ent in doc.ents:\n",
    "        if str(ent) not in entity_dict:\n",
    "            entity_dict[ent.lemma_] = (ent.root.pos_, ent.label_)\n",
    "\n",
    "    # non_entity_strings = [token for token in doc \n",
    "    #                     if token.text not in entity_dict \n",
    "    #                     and token.ent_iob_ == \"O\"\n",
    "    #                     and token.pos_ != 'SPACE']\n",
    "    # entity_dict.update({token.lemma_: (token.pos_, None) for token in non_entity_strings})\n",
    "\n",
    "    return entity_dict\n",
    "\n",
    "small_data['entity_dict'] = small_data['text_filtered'].apply(process_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the filtered text and tokens in its own dataframe and save to csv\n",
    "small_data_filtered = small_data[['text_filtered', 'label',]]\n",
    "small_data_filtered.to_csv('datasets/small_data_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# split the data into training and testing sets\n",
    "train, test = train_test_split(small_data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_features = []\n",
    "train_features_embeddes = []\n",
    "train_features_text_and_tokens = []\n",
    "train_features_entities = []\n",
    "train_features_no_er = []\n",
    "train_labels = []\n",
    "test_features = []\n",
    "test_features_embeddes = []\n",
    "test_features_text_and_tokens = []\n",
    "test_features_entities = []\n",
    "test_features_no_er = []\n",
    "test_labels = []\n",
    "\n",
    "for index, row in train.iterrows():\n",
    "    entity_dict = row['entity_dict']\n",
    "    word_embeddings = np.array(row['text_embeddings'])  # Convert list to numpy array\n",
    "    word_embeddings_doc = np.mean(word_embeddings, axis=0)  # Average the word embeddings\n",
    "    text_filtered = row['text_filtered']\n",
    "    tokens_filtered = row['tokens_filtered']\n",
    "    label = row['label']\n",
    "    \n",
    "    # All features\n",
    "    features = {\n",
    "        'entity_dict': str(entity_dict),  # Convert dictionary to string\n",
    "        'word_embeddings': [str(embedding) for embedding in word_embeddings.tolist()],  # Convert numpy array to list of strings\n",
    "        'text_filtered': str(text_filtered),  # Convert token to string\n",
    "        'tokens_filtered': [str(token) for token in tokens_filtered]  # Convert tokens to strings\n",
    "    }\n",
    "    train_features.append(features)\n",
    "    \n",
    "    # Only embeddings\n",
    "    features_embedds = {\n",
    "        'word_embeddings': [str(embedding) for embedding in word_embeddings.tolist()],  # Convert numpy array to list of strings\n",
    "    } \n",
    "    train_features_embeddes.append(features_embedds)\n",
    "    \n",
    "    # Only text and tokens\n",
    "    features_text_and_tokens = {\n",
    "        'text_filtered': str(text_filtered),  # Convert token to string\n",
    "        'tokens_filtered': [str(token) for token in tokens_filtered]  # Convert tokens to strings\n",
    "    }\n",
    "    train_features_text_and_tokens.append(features_text_and_tokens)\n",
    "    \n",
    "    # Only entities\n",
    "    features_entities = {\n",
    "        'entity_dict': str(entity_dict),  # Convert dictionary to string\n",
    "    }\n",
    "    train_features_entities.append(features_entities)\n",
    "    \n",
    "    # No NER\n",
    "    features_no_ner = {\n",
    "        'word_embeddings': [str(embedding) for embedding in word_embeddings.tolist()],  # Convert numpy array to list of strings\n",
    "        'text_filtered': str(text_filtered),  # Convert token to string\n",
    "        'tokens_filtered': [str(token) for token in tokens_filtered]  # Convert tokens to strings\n",
    "    } \n",
    "    train_features_no_er.append(features_no_ner)\n",
    "    \n",
    "    train_labels.append(label)\n",
    "    \n",
    "for index, row in test.iterrows():\n",
    "    entity_dict = row['entity_dict']\n",
    "    word_embeddings = np.array(row['text_embeddings'])  # Convert list to numpy array\n",
    "    word_embeddings_doc = np.mean(word_embeddings, axis=0)  # Average the word embeddings\n",
    "    text_filtered = row['text_filtered']\n",
    "    tokens_filtered = row['tokens_filtered']\n",
    "    label = row['label']\n",
    "    \n",
    "    features_test = {\n",
    "        'entity_dict': str(entity_dict),  # Convert dictionary to string\n",
    "        'word_embeddings': [str(embedding) for embedding in word_embeddings.tolist()],  # Convert numpy array to list of strings\n",
    "        'text_filtered': str(text_filtered),  # Convert token to string\n",
    "        'tokens_filtered': [str(token) for token in tokens_filtered]  # Convert tokens to strings\n",
    "    }\n",
    "    test_features.append(features_test)\n",
    "    \n",
    "    # Only embeddings\n",
    "    features_embedds = {\n",
    "        'word_embeddings': [str(embedding) for embedding in word_embeddings.tolist()],  # Convert numpy array to list of strings\n",
    "    } \n",
    "    test_features_embeddes.append(features_embedds)\n",
    "    \n",
    "    # Only text and tokens\n",
    "    features_text_and_tokens = {\n",
    "        'text_filtered': str(text_filtered),  # Convert token to string\n",
    "        'tokens_filtered': [str(token) for token in tokens_filtered]  # Convert tokens to strings\n",
    "    }\n",
    "    test_features_text_and_tokens.append(features_text_and_tokens)\n",
    "    \n",
    "    # Only entities\n",
    "    features_entities = {\n",
    "        'entity_dict': str(entity_dict),  # Convert dictionary to string\n",
    "    }\n",
    "    test_features_entities.append(features_entities)\n",
    "    \n",
    "    # No NER\n",
    "    features_no_ner = {\n",
    "        'word_embeddings': [str(embedding) for embedding in word_embeddings.tolist()],  # Convert numpy array to list of strings\n",
    "        'text_filtered': str(text_filtered),  # Convert token to string\n",
    "        'tokens_filtered': [str(token) for token in tokens_filtered]  # Convert tokens to strings\n",
    "    } \n",
    "    test_features_no_er.append(features_no_ner)\n",
    "    test_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def modelFit(model, param_grid, train_features, test_features):\n",
    "    pipeline = Pipeline([\n",
    "        ('vectorizer', DictVectorizer()),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "\n",
    "    # Create the GridSearchCV object\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=10, scoring='accuracy')\n",
    "\n",
    "    # Fit the GridSearchCV object\n",
    "    grid_search.fit(train_features, train_labels)\n",
    "\n",
    "    # Get the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Predict the test data\n",
    "    preds = best_model.predict(test_features)\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    accuracy = accuracy_score(test_labels, preds)\n",
    "    f1 = f1_score(test_labels, preds, average='weighted')\n",
    "    precision = precision_score(test_labels, preds, average='weighted')\n",
    "    recall = recall_score(test_labels, preds, average='weighted')\n",
    "\n",
    "    results = {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'confusion_matrix': confusion_matrix(test_labels, preds),\n",
    "        'best_params': grid_search.best_params_\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(MultinomialNB(), {}, train_features, test_features)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only text and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(MultinomialNB(), {}, train_features_text_and_tokens, test_features_text_and_tokens)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only entities and PoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(MultinomialNB(), {}, train_features_entities, test_features_entities)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Dense Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(MultinomialNB(), {}, train_features_embeddes, test_features_embeddes)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(MultinomialNB(), {}, train_features_no_er, test_features_no_er)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(LogisticRegression(), {\n",
    "    'classifier__C': [0.1, 1.0, 10.0],\n",
    "    'classifier__penalty': ['l1', 'l2'],\n",
    "    'classifier__solver': ['liblinear', 'saga']\n",
    "}, train_features, test_features)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "results = modelFit(DecisionTreeClassifier(), {\n",
    "        'classifier__max_depth': [None, 5, 10, 15],\n",
    "        'classifier__min_samples_split': [2, 5, 10]\n",
    "    }, train_features, test_features)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
