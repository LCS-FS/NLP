{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11104</th>\n",
       "      <td>Chiron Delays Flu Vaccine Shipments  NEW YORK ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92576</th>\n",
       "      <td>Sheffield tops list of AL MVP favourites CBC S...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5531</th>\n",
       "      <td>Red Sox: Burly effort CHICAGO -- A year ago la...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14492</th>\n",
       "      <td>Hewitt serves notice heading to Open When you ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96578</th>\n",
       "      <td>Dollar declines following Greenspan remarks (A...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "11104  Chiron Delays Flu Vaccine Shipments  NEW YORK ...      2\n",
       "92576  Sheffield tops list of AL MVP favourites CBC S...      1\n",
       "5531   Red Sox: Burly effort CHICAGO -- A year ago la...      1\n",
       "14492  Hewitt serves notice heading to Open When you ...      1\n",
       "96578  Dollar declines following Greenspan remarks (A...      0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('datasets/training_data.csv')\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = ' '.join(data['text'])\n",
    "# print(corpus[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of unique tokens in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tokenize import wordpunct_tokenize\n",
    "# from collections import Counter\n",
    "\n",
    "# tokens = wordpunct_tokenize(corpus)\n",
    "# word_counts = Counter(tokens)\n",
    "\n",
    "# print(\"Number of unique tokens:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk import word_tokenize, sent_tokenize\n",
    "# from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "# data['word_tokens'] = data['text'].apply(word_tokenize) # slower than wordpunct_tokenize\n",
    "# data['word_tokens'] = data['text'].apply(wordpunct_tokenize)\n",
    "# data['word_counts'] = data['word_tokens'].apply(lambda x: len(x))\n",
    "# data['unique_word_counts'] = data['word_tokens'].apply(lambda x: len(set(x)))\n",
    "\n",
    "# data['sentences'] = data['text'].apply(sent_tokenize)\n",
    "# data['sentence_counts'] = data['sentences'].apply(len)\n",
    "\n",
    "# data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Customize pipeline\n",
    "# nlp.remove_pipe('tok2vec')\n",
    "# nlp.remove_pipe('tagger')\n",
    "# nlp.remove_pipe('parser')\n",
    "# nlp.remove_pipe('attribute_ruler')\n",
    "# nlp.remove_pipe('lemmatizer')\n",
    "# nlp.remove_pipe('ner')\n",
    "\n",
    "nlp.enable_pipe('senter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# from spacy.tokens import Doc\n",
    "# from spacy.language import Language\n",
    "\n",
    "# @Language.component(\"regex_cleanup\")\n",
    "# def regex_cleanup(doc):\n",
    "#     cleaned_tokens = []\n",
    "#     for token in doc:\n",
    "#         if not re.match(r'((\\w+))', token.text):\n",
    "#             cleaned_tokens.append(token)\n",
    "#     return Doc(doc.vocab, words=[token.text for token in cleaned_tokens])\n",
    "\n",
    "# nlp.add_pipe(\"regex_cleanup\", first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53573</th>\n",
       "      <td>Labor Costs, Hurricane Hit Alcoa Profit Alcoa ...</td>\n",
       "      <td>2</td>\n",
       "      <td>(Labor, Costs, ,, Hurricane, Hit, Alcoa, Profi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113311</th>\n",
       "      <td>Free agent Pavano chooses Yanks Carl Pavano ha...</td>\n",
       "      <td>1</td>\n",
       "      <td>(Free, agent, Pavano, chooses, Yanks, Carl, Pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17593</th>\n",
       "      <td>Second quarter sees rise in storage revenue Wo...</td>\n",
       "      <td>3</td>\n",
       "      <td>(Second, quarter, sees, rise, in, storage, rev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80083</th>\n",
       "      <td>Hewitt holds off French talent Monfils in Pari...</td>\n",
       "      <td>1</td>\n",
       "      <td>(Hewitt, holds, off, French, talent, Monfils, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44861</th>\n",
       "      <td>Baghdad hit by several bomb blasts A suicide b...</td>\n",
       "      <td>0</td>\n",
       "      <td>(Baghdad, hit, by, several, bomb, blasts, A, s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label  \\\n",
       "53573   Labor Costs, Hurricane Hit Alcoa Profit Alcoa ...      2   \n",
       "113311  Free agent Pavano chooses Yanks Carl Pavano ha...      1   \n",
       "17593   Second quarter sees rise in storage revenue Wo...      3   \n",
       "80083   Hewitt holds off French talent Monfils in Pari...      1   \n",
       "44861   Baghdad hit by several bomb blasts A suicide b...      0   \n",
       "\n",
       "                                                   tokens  \n",
       "53573   (Labor, Costs, ,, Hurricane, Hit, Alcoa, Profi...  \n",
       "113311  (Free, agent, Pavano, chooses, Yanks, Carl, Pa...  \n",
       "17593   (Second, quarter, sees, rise, in, storage, rev...  \n",
       "80083   (Hewitt, holds, off, French, talent, Monfils, ...  \n",
       "44861   (Baghdad, hit, by, several, bomb, blasts, A, s...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: process all data\n",
    "small_data = data.sample(1000)\n",
    "small_data['tokens'] = small_data['text'].apply(nlp)\n",
    "small_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1000.000000\n",
       "mean       44.316000\n",
       "std        11.614346\n",
       "min        16.000000\n",
       "25%        37.000000\n",
       "50%        44.000000\n",
       "75%        50.000000\n",
       "max       132.000000\n",
       "Name: tokens_count, dtype: float64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data['tokens_count'] = small_data['tokens'].apply(len)\n",
    "small_data['tokens_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tokens(tokens):\n",
    "    tokens_without_punctuation = [token for token in tokens if not token.is_punct]\n",
    "    tokens_without_space = [token for token in tokens_without_punctuation if not token.is_space]\n",
    "    tokens_without_stopwords = [token for token in tokens_without_space if not token.is_stop]\n",
    "    return tokens_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text(text):\n",
    "    tokens_lemmatized = [token.lemma_ for token in text]\n",
    "    tokens_lower = [token.lower() for token in tokens_lemmatized]\n",
    "    return ' '.join(tokens_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_embeddings(text):\n",
    "    token_embeddings = [token.vector for token in text]\n",
    "    return token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_ner(text):\n",
    "    return [(token, token.pos_, token.ent_iob_, token.ent_type_) for token in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_count</th>\n",
       "      <th>tokens_filtered</th>\n",
       "      <th>text_filtered</th>\n",
       "      <th>text_embeddings</th>\n",
       "      <th>text_ner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>92274</th>\n",
       "      <td>NFL : McNabb, Owens torch Cowboys While it was...</td>\n",
       "      <td>1</td>\n",
       "      <td>(NFL, :, McNabb, ,, Owens, torch, Cowboys, Whi...</td>\n",
       "      <td>44</td>\n",
       "      <td>[NFL, McNabb, Owens, torch, Cowboys, sure, hit...</td>\n",
       "      <td>nfl mcnabb owens torch cowboys sure hit east c...</td>\n",
       "      <td>[[-4.2266, 8.2051, -1.2351, 4.3285, 6.7409, 11...</td>\n",
       "      <td>[(NFL, NOUN, B, ORG), (McNabb, PROPN, B, PERSO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44181</th>\n",
       "      <td>Bittersweet day in Montreal as Expos officiall...</td>\n",
       "      <td>0</td>\n",
       "      <td>(Bittersweet, day, in, Montreal, as, Expos, of...</td>\n",
       "      <td>35</td>\n",
       "      <td>[Bittersweet, day, Montreal, Expos, officially...</td>\n",
       "      <td>bittersweet day montreal expos officially play...</td>\n",
       "      <td>[[0.73147, -2.604, -3.4392, 0.035654, -2.0559,...</td>\n",
       "      <td>[(Bittersweet, PROPN, O, ), (day, NOUN, O, ), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106304</th>\n",
       "      <td>Snake Eater a Slithering Success Despite all t...</td>\n",
       "      <td>3</td>\n",
       "      <td>(Snake, Eater, a, Slithering, Success, Despite...</td>\n",
       "      <td>38</td>\n",
       "      <td>[Snake, Eater, Slithering, Success, Despite, B...</td>\n",
       "      <td>snake eater slither success despite bond touch...</td>\n",
       "      <td>[[-2.0681, 1.4007, -0.4877, -3.2356, 0.53422, ...</td>\n",
       "      <td>[(Snake, PROPN, O, ), (Eater, PROPN, O, ), (Sl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44583</th>\n",
       "      <td>IBM claims BlueGene/L now world #39;s most pow...</td>\n",
       "      <td>3</td>\n",
       "      <td>(IBM, claims, BlueGene, /, L, now, world, #, 3...</td>\n",
       "      <td>41</td>\n",
       "      <td>[IBM, claims, BlueGene, L, world, 39;s, powerf...</td>\n",
       "      <td>ibm claim bluegene l world 39;s powerful super...</td>\n",
       "      <td>[[-6.2718, -2.105, 8.7398, 4.6458, 2.0883, 1.5...</td>\n",
       "      <td>[(IBM, PROPN, B, ORG), (claims, VERB, O, ), (B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73653</th>\n",
       "      <td>HP, QLogic Unveil SAN-in-a-Box Storage powerho...</td>\n",
       "      <td>3</td>\n",
       "      <td>(HP, ,, QLogic, Unveil, SAN, -, in, -, a, -, B...</td>\n",
       "      <td>43</td>\n",
       "      <td>[HP, QLogic, Unveil, SAN, Box, Storage, powerh...</td>\n",
       "      <td>hp qlogic unveil san box storage powerhouse he...</td>\n",
       "      <td>[[-0.79661, 8.0083, -4.6256, 7.6264, 0.02344, ...</td>\n",
       "      <td>[(HP, PROPN, B, ORG), (QLogic, PROPN, B, ORG),...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label  \\\n",
       "92274   NFL : McNabb, Owens torch Cowboys While it was...      1   \n",
       "44181   Bittersweet day in Montreal as Expos officiall...      0   \n",
       "106304  Snake Eater a Slithering Success Despite all t...      3   \n",
       "44583   IBM claims BlueGene/L now world #39;s most pow...      3   \n",
       "73653   HP, QLogic Unveil SAN-in-a-Box Storage powerho...      3   \n",
       "\n",
       "                                                   tokens  tokens_count  \\\n",
       "92274   (NFL, :, McNabb, ,, Owens, torch, Cowboys, Whi...            44   \n",
       "44181   (Bittersweet, day, in, Montreal, as, Expos, of...            35   \n",
       "106304  (Snake, Eater, a, Slithering, Success, Despite...            38   \n",
       "44583   (IBM, claims, BlueGene, /, L, now, world, #, 3...            41   \n",
       "73653   (HP, ,, QLogic, Unveil, SAN, -, in, -, a, -, B...            43   \n",
       "\n",
       "                                          tokens_filtered  \\\n",
       "92274   [NFL, McNabb, Owens, torch, Cowboys, sure, hit...   \n",
       "44181   [Bittersweet, day, Montreal, Expos, officially...   \n",
       "106304  [Snake, Eater, Slithering, Success, Despite, B...   \n",
       "44583   [IBM, claims, BlueGene, L, world, 39;s, powerf...   \n",
       "73653   [HP, QLogic, Unveil, SAN, Box, Storage, powerh...   \n",
       "\n",
       "                                            text_filtered  \\\n",
       "92274   nfl mcnabb owens torch cowboys sure hit east c...   \n",
       "44181   bittersweet day montreal expos officially play...   \n",
       "106304  snake eater slither success despite bond touch...   \n",
       "44583   ibm claim bluegene l world 39;s powerful super...   \n",
       "73653   hp qlogic unveil san box storage powerhouse he...   \n",
       "\n",
       "                                          text_embeddings  \\\n",
       "92274   [[-4.2266, 8.2051, -1.2351, 4.3285, 6.7409, 11...   \n",
       "44181   [[0.73147, -2.604, -3.4392, 0.035654, -2.0559,...   \n",
       "106304  [[-2.0681, 1.4007, -0.4877, -3.2356, 0.53422, ...   \n",
       "44583   [[-6.2718, -2.105, 8.7398, 4.6458, 2.0883, 1.5...   \n",
       "73653   [[-0.79661, 8.0083, -4.6256, 7.6264, 0.02344, ...   \n",
       "\n",
       "                                                 text_ner  \n",
       "92274   [(NFL, NOUN, B, ORG), (McNabb, PROPN, B, PERSO...  \n",
       "44181   [(Bittersweet, PROPN, O, ), (day, NOUN, O, ), ...  \n",
       "106304  [(Snake, PROPN, O, ), (Eater, PROPN, O, ), (Sl...  \n",
       "44583   [(IBM, PROPN, B, ORG), (claims, VERB, O, ), (B...  \n",
       "73653   [(HP, PROPN, B, ORG), (QLogic, PROPN, B, ORG),...  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data['tokens_filtered'] = small_data['tokens'].apply(filter_tokens)\n",
    "small_data['text_filtered'] = small_data['tokens_filtered'].apply(filter_text)\n",
    "small_data['text_embeddings'] = small_data['tokens_filtered'].apply(text_embeddings)\n",
    "small_data['text_ner'] = small_data['tokens_filtered'].apply(text_ner)\n",
    "small_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_entities(text_filtered):\n",
    "    entity_dict = {}\n",
    "    doc = nlp(text_filtered)\n",
    "    for ent in doc.ents:\n",
    "        if str(ent) not in entity_dict:\n",
    "            entity_dict[ent.lemma_] = (ent.root.pos_, ent.label_)\n",
    "\n",
    "    non_entity_strings = [token for token in doc \n",
    "                        if token.text not in entity_dict \n",
    "                        and token.ent_iob_ == \"O\"\n",
    "                        and token.pos_ != 'SPACE']\n",
    "    entity_dict.update({token.lemma_: (token.pos_, None) for token in non_entity_strings})\n",
    "\n",
    "    return entity_dict\n",
    "\n",
    "small_data['entity_dict'] = small_data['text_filtered'].apply(process_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_count</th>\n",
       "      <th>tokens_filtered</th>\n",
       "      <th>text_filtered</th>\n",
       "      <th>text_embeddings</th>\n",
       "      <th>text_ner</th>\n",
       "      <th>entity_dict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80934</th>\n",
       "      <td>Bomber Hits Near Baghdad Airport A car bomb ex...</td>\n",
       "      <td>0</td>\n",
       "      <td>(Bomber, Hits, Near, Baghdad, Airport, A, car,...</td>\n",
       "      <td>37</td>\n",
       "      <td>[Bomber, Hits, Near, Baghdad, Airport, car, bo...</td>\n",
       "      <td>bomber hits near baghdad airport car bomb expl...</td>\n",
       "      <td>[[-0.1339, 2.3994, -0.88813, 2.1828, -0.51505,...</td>\n",
       "      <td>[(Bomber, PROPN, O, ), (Hits, PROPN, O, ), (Ne...</td>\n",
       "      <td>{'baghdad airport': ('PROPN', 'FAC'), 'lebanes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101886</th>\n",
       "      <td>Dollar Hits New Low; Gold Heads for  #36;455 (...</td>\n",
       "      <td>2</td>\n",
       "      <td>(Dollar, Hits, New, Low, ;, Gold, Heads, for, ...</td>\n",
       "      <td>57</td>\n",
       "      <td>[Dollar, Hits, New, Low, Gold, Heads, 36;455, ...</td>\n",
       "      <td>dollar hits new low gold heads 36;455 reuters ...</td>\n",
       "      <td>[[-1.8113, 0.44784, -1.846, 0.52974, 0.78695, ...</td>\n",
       "      <td>[(Dollar, NOUN, O, ), (Hits, PROPN, O, ), (New...</td>\n",
       "      <td>{'36;455': ('CCONJ', 'CARDINAL'), 'reuter reut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14941</th>\n",
       "      <td>Okocha Tests Liverpool Super Eagles captain Au...</td>\n",
       "      <td>1</td>\n",
       "      <td>(Okocha, Tests, Liverpool, Super, Eagles, capt...</td>\n",
       "      <td>36</td>\n",
       "      <td>[Okocha, Tests, Liverpool, Super, Eagles, capt...</td>\n",
       "      <td>okocha tests liverpool super eagles captain au...</td>\n",
       "      <td>[[1.5999, 3.4158, 1.0194, -0.1812, 0.40594, -1...</td>\n",
       "      <td>[(Okocha, PROPN, B, ORG), (Tests, PROPN, I, OR...</td>\n",
       "      <td>{'okocha': ('PROPN', 'NORP'), 'liverpool super...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116709</th>\n",
       "      <td>Sorry Anelka still on the trading block Anelka...</td>\n",
       "      <td>1</td>\n",
       "      <td>(Sorry, Anelka, still, on, the, trading, block...</td>\n",
       "      <td>42</td>\n",
       "      <td>[Sorry, Anelka, trading, block, Anelka, expres...</td>\n",
       "      <td>sorry anelka trading block anelka express wish...</td>\n",
       "      <td>[[4.1412, 1.4775, 1.7142, -1.2661, -2.878, -1....</td>\n",
       "      <td>[(Sorry, INTJ, O, ), (Anelka, PROPN, B, PRODUC...</td>\n",
       "      <td>{'anelka': ('PROPN', 'NORP'), 'quot;big': ('PR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105818</th>\n",
       "      <td>Dollar Down But Seen in Ranges (Reuters) Reute...</td>\n",
       "      <td>2</td>\n",
       "      <td>(Dollar, Down, But, Seen, in, Ranges, (, Reute...</td>\n",
       "      <td>44</td>\n",
       "      <td>[Dollar, Seen, Ranges, Reuters, Reuters, dolla...</td>\n",
       "      <td>dollar see ranges reuters reuters dollar fall ...</td>\n",
       "      <td>[[-1.8113, 0.44784, -1.846, 0.52974, 0.78695, ...</td>\n",
       "      <td>[(Dollar, NOUN, O, ), (Seen, VERB, O, ), (Rang...</td>\n",
       "      <td>{'reuters reuters': ('PROPN', 'ORG'), 'dollar ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label  \\\n",
       "80934   Bomber Hits Near Baghdad Airport A car bomb ex...      0   \n",
       "101886  Dollar Hits New Low; Gold Heads for  #36;455 (...      2   \n",
       "14941   Okocha Tests Liverpool Super Eagles captain Au...      1   \n",
       "116709  Sorry Anelka still on the trading block Anelka...      1   \n",
       "105818  Dollar Down But Seen in Ranges (Reuters) Reute...      2   \n",
       "\n",
       "                                                   tokens  tokens_count  \\\n",
       "80934   (Bomber, Hits, Near, Baghdad, Airport, A, car,...            37   \n",
       "101886  (Dollar, Hits, New, Low, ;, Gold, Heads, for, ...            57   \n",
       "14941   (Okocha, Tests, Liverpool, Super, Eagles, capt...            36   \n",
       "116709  (Sorry, Anelka, still, on, the, trading, block...            42   \n",
       "105818  (Dollar, Down, But, Seen, in, Ranges, (, Reute...            44   \n",
       "\n",
       "                                          tokens_filtered  \\\n",
       "80934   [Bomber, Hits, Near, Baghdad, Airport, car, bo...   \n",
       "101886  [Dollar, Hits, New, Low, Gold, Heads, 36;455, ...   \n",
       "14941   [Okocha, Tests, Liverpool, Super, Eagles, capt...   \n",
       "116709  [Sorry, Anelka, trading, block, Anelka, expres...   \n",
       "105818  [Dollar, Seen, Ranges, Reuters, Reuters, dolla...   \n",
       "\n",
       "                                            text_filtered  \\\n",
       "80934   bomber hits near baghdad airport car bomb expl...   \n",
       "101886  dollar hits new low gold heads 36;455 reuters ...   \n",
       "14941   okocha tests liverpool super eagles captain au...   \n",
       "116709  sorry anelka trading block anelka express wish...   \n",
       "105818  dollar see ranges reuters reuters dollar fall ...   \n",
       "\n",
       "                                          text_embeddings  \\\n",
       "80934   [[-0.1339, 2.3994, -0.88813, 2.1828, -0.51505,...   \n",
       "101886  [[-1.8113, 0.44784, -1.846, 0.52974, 0.78695, ...   \n",
       "14941   [[1.5999, 3.4158, 1.0194, -0.1812, 0.40594, -1...   \n",
       "116709  [[4.1412, 1.4775, 1.7142, -1.2661, -2.878, -1....   \n",
       "105818  [[-1.8113, 0.44784, -1.846, 0.52974, 0.78695, ...   \n",
       "\n",
       "                                                 text_ner  \\\n",
       "80934   [(Bomber, PROPN, O, ), (Hits, PROPN, O, ), (Ne...   \n",
       "101886  [(Dollar, NOUN, O, ), (Hits, PROPN, O, ), (New...   \n",
       "14941   [(Okocha, PROPN, B, ORG), (Tests, PROPN, I, OR...   \n",
       "116709  [(Sorry, INTJ, O, ), (Anelka, PROPN, B, PRODUC...   \n",
       "105818  [(Dollar, NOUN, O, ), (Seen, VERB, O, ), (Rang...   \n",
       "\n",
       "                                              entity_dict  \n",
       "80934   {'baghdad airport': ('PROPN', 'FAC'), 'lebanes...  \n",
       "101886  {'36;455': ('CCONJ', 'CARDINAL'), 'reuter reut...  \n",
       "14941   {'okocha': ('PROPN', 'NORP'), 'liverpool super...  \n",
       "116709  {'anelka': ('PROPN', 'NORP'), 'quot;big': ('PR...  \n",
       "105818  {'reuters reuters': ('PROPN', 'ORG'), 'dollar ...  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the filtered text and tokens in its own dataframe and save to csv\n",
    "small_data_filtered = small_data[['text_filtered', 'label',]]\n",
    "small_data_filtered.to_csv('datasets/small_data_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %store small_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8600\n",
      "F1 Score: 0.8596\n",
      "Precision: 0.8635\n",
      "Recall: 0.8600\n",
      "[[36  0  4  2]\n",
      " [ 1 44  1  0]\n",
      " [ 2  0 58  5]\n",
      " [ 1  1 11 34]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# split the data into training and testing sets\n",
    "train, test = train_test_split(small_data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_features = []\n",
    "train_labels = []\n",
    "test_features = []\n",
    "test_labels = []\n",
    "\n",
    "for index, row in train.iterrows():\n",
    "    entity_dict = row['entity_dict']\n",
    "    word_embeddings = np.array(row['text_embeddings'])  # Convert list to numpy array\n",
    "    word_embeddings_doc = np.mean(word_embeddings, axis=0)  # Average the word embeddings\n",
    "    text_filtered = row['text_filtered']\n",
    "    tokens_filtered = row['tokens_filtered']\n",
    "    label = row['label']\n",
    "    \n",
    "    features = {\n",
    "        'entity_dict': str(entity_dict),  # Convert dictionary to string\n",
    "        'word_embeddings': [str(embedding) for embedding in word_embeddings.tolist()],  # Convert numpy array to list of strings\n",
    "        'text_filtered': str(text_filtered),  # Convert token to string\n",
    "        'tokens_filtered': [str(token) for token in tokens_filtered]  # Convert tokens to strings\n",
    "    }\n",
    "    train_features.append(features)\n",
    "    train_labels.append(label)\n",
    "    \n",
    "for index, row in test.iterrows():\n",
    "    entity_dict = row['entity_dict']\n",
    "    word_embeddings = np.array(row['text_embeddings'])  # Convert list to numpy array\n",
    "    word_embeddings_doc = np.mean(word_embeddings, axis=0)  # Average the word embeddings\n",
    "    text_filtered = row['text_filtered']\n",
    "    tokens_filtered = row['tokens_filtered']\n",
    "    label = row['label']\n",
    "    \n",
    "    features = {\n",
    "        'entity_dict': str(entity_dict),  # Convert dictionary to string\n",
    "        'word_embeddings': [str(embedding) for embedding in word_embeddings.tolist()],  # Convert numpy array to list of strings\n",
    "        'text_filtered': str(text_filtered),  # Convert token to string\n",
    "        'tokens_filtered': [str(token) for token in tokens_filtered]  # Convert tokens to strings\n",
    "    }\n",
    "    test_features.append(features)\n",
    "    test_labels.append(label)\n",
    "\n",
    "# Define a pipeline with DictVectorizer and Logistic Regression\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', DictVectorizer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(train_features, train_labels)\n",
    "\n",
    "# evaluate the model in accuracy and f1 score and precision and recall\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Predict the test data\n",
    "preds = pipeline.predict(test_features)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(test_labels, preds)\n",
    "f1 = f1_score(test_labels, preds, average='weighted')\n",
    "precision = precision_score(test_labels, preds, average='weighted')\n",
    "recall = recall_score(test_labels, preds, average='weighted')\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "\n",
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# confusion matrix\n",
    "conf_matrix = confusion_matrix(test_labels, preds)\n",
    "print(conf_matrix)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
