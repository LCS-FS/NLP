{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59375</th>\n",
       "      <td>Yankees 10, Red Sox 7: Just a typical chapter ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54767</th>\n",
       "      <td>Alpharma Sells Generic of Pfizer Drug  NEW YOR...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34472</th>\n",
       "      <td>Guerrero Beaned During Angels Game (Reuters) R...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11645</th>\n",
       "      <td>O Romeo, art thou a good Republican? Romance f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71008</th>\n",
       "      <td>21 die, 2000 injured in Japan #39;s quake Loca...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "59375  Yankees 10, Red Sox 7: Just a typical chapter ...      1\n",
       "54767  Alpharma Sells Generic of Pfizer Drug  NEW YOR...      2\n",
       "34472  Guerrero Beaned During Angels Game (Reuters) R...      1\n",
       "11645  O Romeo, art thou a good Republican? Romance f...      0\n",
       "71008  21 die, 2000 injured in Japan #39;s quake Loca...      0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('datasets/training_data.csv')\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = ' '.join(data['text'])\n",
    "# print(corpus[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not perfect, there's instances like '(Reuters)' that are not removed\n",
    "def remove_reuters(text):\n",
    "    pattern = r'\\((\\w+)\\) \\1+'\n",
    "    return re.sub(pattern, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(text):\n",
    "    # Example\n",
    "    # &lt;A HREF=\"\"http://www.investor.reuters.com/FullQuote.aspx?ticker=BNNY.OB target=/stocks/quickinfo/fullquote\"\"&gt;BNNY.OB&lt;/A&gt;\n",
    "    pattern = r'&lt;A HREF=.*?&gt;(.*?)&lt;/A&gt;'\n",
    "    return re.sub(pattern, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    decoded_text = html.unescape(text)\n",
    "    pattern = r'<.*?>'\n",
    "    return re.sub(pattern, '', decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_quotes(text):\n",
    "    pattern = r'quot;'\n",
    "    return re.sub(pattern, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Somewhere between gleam and gloom President Bush has been saying that the US economy has ''turned the corner.  Democratic presidential candidate Senator John F. Kerry, in the wake of this month's poor jobs report, quipped that it was more like a U-turn.\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'] = data['text'].apply(remove_links)\n",
    "data['text'] = data['text'].apply(remove_reuters)\n",
    "data['text'] = data['text'].apply(remove_html_tags)\n",
    "data['text'] = data['text'].apply(remove_quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wall St. Bears Claw Back Into the Black  - Sho...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace  - P...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook  - Soari...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Wall St. Bears Claw Back Into the Black  - Sho...      2\n",
       "1  Carlyle Looks Toward Commercial Aerospace  - P...      2\n",
       "2  Oil and Economy Cloud Stocks' Outlook  - Soari...      2\n",
       "3  Iraq Halts Oil Exports from Main Southern Pipe...      2\n",
       "4  Oil prices soar to all-time record, posing new...      2"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Customize pipeline\n",
    "# nlp.remove_pipe('tok2vec')\n",
    "# nlp.remove_pipe('tagger')\n",
    "# nlp.remove_pipe('parser')\n",
    "# nlp.remove_pipe('attribute_ruler')\n",
    "# nlp.remove_pipe('lemmatizer')\n",
    "# nlp.remove_pipe('ner')\n",
    "\n",
    "# nlp.enable_pipe('senter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gdrp7\\AppData\\Local\\Temp\\ipykernel_2312\\2375748365.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  small_data['tokens'] = small_data['text'].apply(nlp)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>Columnists: The Panacea of Information Securit...</td>\n",
       "      <td>3</td>\n",
       "      <td>(Columnists, :, The, Panacea, of, Information,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>U.N. Official Urges Political Independence  - ...</td>\n",
       "      <td>0</td>\n",
       "      <td>(U.N., Official, Urges, Political, Independenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>Rescue Teams Aid Hurricane Charley Victims PUN...</td>\n",
       "      <td>0</td>\n",
       "      <td>(Rescue, Teams, Aid, Hurricane, Charley, Victi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Dollar Rises Vs Euro After Asset Data  NEW YOR...</td>\n",
       "      <td>2</td>\n",
       "      <td>(Dollar, Rises, Vs, Euro, After, Asset, Data, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>The Region's Highest-Paid Executives Pay for t...</td>\n",
       "      <td>2</td>\n",
       "      <td>(The, Region, 's, Highest, -, Paid, Executives...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label  \\\n",
       "436  Columnists: The Panacea of Information Securit...      3   \n",
       "952  U.N. Official Urges Political Independence  - ...      0   \n",
       "777  Rescue Teams Aid Hurricane Charley Victims PUN...      0   \n",
       "996  Dollar Rises Vs Euro After Asset Data  NEW YOR...      2   \n",
       "669  The Region's Highest-Paid Executives Pay for t...      2   \n",
       "\n",
       "                                                tokens  \n",
       "436  (Columnists, :, The, Panacea, of, Information,...  \n",
       "952  (U.N., Official, Urges, Political, Independenc...  \n",
       "777  (Rescue, Teams, Aid, Hurricane, Charley, Victi...  \n",
       "996  (Dollar, Rises, Vs, Euro, After, Asset, Data, ...  \n",
       "669  (The, Region, 's, Highest, -, Paid, Executives...  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: process all data\n",
    "small_data = data.head(1000)\n",
    "small_data['tokens'] = small_data['text'].apply(nlp)\n",
    "small_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Wall St. Bears Claw Back Into the Black  - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data.iloc[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gdrp7\\AppData\\Local\\Temp\\ipykernel_2312\\1130808158.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  small_data['tokens_count'] = small_data['tokens'].apply(len)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    1000.000000\n",
       "mean       46.372000\n",
       "std        20.876864\n",
       "min        14.000000\n",
       "25%        31.000000\n",
       "50%        44.000000\n",
       "75%        55.000000\n",
       "max       152.000000\n",
       "Name: tokens_count, dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data['tokens_count'] = small_data['tokens'].apply(len)\n",
    "small_data['tokens_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tokens(tokens):\n",
    "    tokens_without_punctuation = [token for token in tokens if not token.is_punct]\n",
    "    tokens_without_space = [token for token in tokens_without_punctuation if not token.is_space]\n",
    "    tokens_without_stopwords = [token for token in tokens_without_space if not token.is_stop]\n",
    "    return tokens_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text(text):\n",
    "    tokens_lemmatized = [token.lemma_ for token in text]\n",
    "    tokens_lower = [token.lower() for token in tokens_lemmatized]\n",
    "    return ' '.join(tokens_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_embeddings(text):\n",
    "    token_embeddings = [token.vector for token in text]\n",
    "    return token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_ner(text):\n",
    "    return [(token, token.pos_, token.ent_iob_, token.ent_type_) for token in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gdrp7\\AppData\\Local\\Temp\\ipykernel_9564\\4248049289.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  small_data['tokens_filtered'] = small_data['tokens'].apply(filter_tokens)\n",
      "C:\\Users\\gdrp7\\AppData\\Local\\Temp\\ipykernel_9564\\4248049289.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  small_data['text_filtered'] = small_data['tokens_filtered'].apply(filter_text)\n",
      "C:\\Users\\gdrp7\\AppData\\Local\\Temp\\ipykernel_9564\\4248049289.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  small_data['text_embeddings'] = small_data['tokens_filtered'].apply(text_embeddings)\n",
      "C:\\Users\\gdrp7\\AppData\\Local\\Temp\\ipykernel_9564\\4248049289.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  small_data['text_ner'] = small_data['tokens_filtered'].apply(text_ner)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_count</th>\n",
       "      <th>tokens_filtered</th>\n",
       "      <th>text_filtered</th>\n",
       "      <th>text_embeddings</th>\n",
       "      <th>text_ner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>Video games 'good for children' Computer games...</td>\n",
       "      <td>3</td>\n",
       "      <td>(Video, games, ', good, for, children, ', Comp...</td>\n",
       "      <td>26</td>\n",
       "      <td>[Video, games, good, children, Computer, games...</td>\n",
       "      <td>video game good child computer game promote pr...</td>\n",
       "      <td>[[-0.057557, -1.7326, 4.388, 2.1707, -1.1465, ...</td>\n",
       "      <td>[(Video, NOUN, O, ), (games, NOUN, O, ), (good...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>Lenovo revenue grows, but problems persist Chi...</td>\n",
       "      <td>3</td>\n",
       "      <td>(Lenovo, revenue, grows, ,, but, problems, per...</td>\n",
       "      <td>24</td>\n",
       "      <td>[Lenovo, revenue, grows, problems, persist, Ch...</td>\n",
       "      <td>lenovo revenue grow problem persist china larg...</td>\n",
       "      <td>[[0.92553, 2.4457, -0.12281, 3.1267, 0.7986, 2...</td>\n",
       "      <td>[(Lenovo, PROPN, B, ORG), (revenue, NOUN, O, )...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>U.S. Barred From Weakening Dolphin Rules (AP) ...</td>\n",
       "      <td>3</td>\n",
       "      <td>(U.S., Barred, From, Weakening, Dolphin, Rules...</td>\n",
       "      <td>50</td>\n",
       "      <td>[U.S., Barred, Weakening, Dolphin, Rules, AP, ...</td>\n",
       "      <td>u.s. bar weaken dolphin rules ap ap victory en...</td>\n",
       "      <td>[[-3.8398, 2.7468, -5.5801, 6.5437, 7.1942, 2....</td>\n",
       "      <td>[(U.S., PROPN, B, GPE), (Barred, VERB, O, ), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>Swimming showdown Ian Thorpe and Michael Phelp...</td>\n",
       "      <td>0</td>\n",
       "      <td>(Swimming, showdown, Ian, Thorpe, and, Michael...</td>\n",
       "      <td>24</td>\n",
       "      <td>[Swimming, showdown, Ian, Thorpe, Michael, Phe...</td>\n",
       "      <td>swimming showdown ian thorpe michael phelps ch...</td>\n",
       "      <td>[[2.9178, -2.1378, 0.85246, -4.7337, 1.4823, 0...</td>\n",
       "      <td>[(Swimming, NOUN, O, ), (showdown, NOUN, O, ),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>Millions Wait Hours in Venezuela to Vote in Re...</td>\n",
       "      <td>0</td>\n",
       "      <td>(Millions, Wait, Hours, in, Venezuela, to, Vot...</td>\n",
       "      <td>35</td>\n",
       "      <td>[Millions, Wait, Hours, Venezuela, Vote, Recal...</td>\n",
       "      <td>million wait hour venezuela vote recall electi...</td>\n",
       "      <td>[[-2.9262, -2.5402, -0.27415, 4.2282, -0.94817...</td>\n",
       "      <td>[(Millions, NOUN, B, CARDINAL), (Wait, VERB, O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label  \\\n",
       "200  Video games 'good for children' Computer games...      3   \n",
       "320  Lenovo revenue grows, but problems persist Chi...      3   \n",
       "134  U.S. Barred From Weakening Dolphin Rules (AP) ...      3   \n",
       "679  Swimming showdown Ian Thorpe and Michael Phelp...      0   \n",
       "629  Millions Wait Hours in Venezuela to Vote in Re...      0   \n",
       "\n",
       "                                                tokens  tokens_count  \\\n",
       "200  (Video, games, ', good, for, children, ', Comp...            26   \n",
       "320  (Lenovo, revenue, grows, ,, but, problems, per...            24   \n",
       "134  (U.S., Barred, From, Weakening, Dolphin, Rules...            50   \n",
       "679  (Swimming, showdown, Ian, Thorpe, and, Michael...            24   \n",
       "629  (Millions, Wait, Hours, in, Venezuela, to, Vot...            35   \n",
       "\n",
       "                                       tokens_filtered  \\\n",
       "200  [Video, games, good, children, Computer, games...   \n",
       "320  [Lenovo, revenue, grows, problems, persist, Ch...   \n",
       "134  [U.S., Barred, Weakening, Dolphin, Rules, AP, ...   \n",
       "679  [Swimming, showdown, Ian, Thorpe, Michael, Phe...   \n",
       "629  [Millions, Wait, Hours, Venezuela, Vote, Recal...   \n",
       "\n",
       "                                         text_filtered  \\\n",
       "200  video game good child computer game promote pr...   \n",
       "320  lenovo revenue grow problem persist china larg...   \n",
       "134  u.s. bar weaken dolphin rules ap ap victory en...   \n",
       "679  swimming showdown ian thorpe michael phelps ch...   \n",
       "629  million wait hour venezuela vote recall electi...   \n",
       "\n",
       "                                       text_embeddings  \\\n",
       "200  [[-0.057557, -1.7326, 4.388, 2.1707, -1.1465, ...   \n",
       "320  [[0.92553, 2.4457, -0.12281, 3.1267, 0.7986, 2...   \n",
       "134  [[-3.8398, 2.7468, -5.5801, 6.5437, 7.1942, 2....   \n",
       "679  [[2.9178, -2.1378, 0.85246, -4.7337, 1.4823, 0...   \n",
       "629  [[-2.9262, -2.5402, -0.27415, 4.2282, -0.94817...   \n",
       "\n",
       "                                              text_ner  \n",
       "200  [(Video, NOUN, O, ), (games, NOUN, O, ), (good...  \n",
       "320  [(Lenovo, PROPN, B, ORG), (revenue, NOUN, O, )...  \n",
       "134  [(U.S., PROPN, B, GPE), (Barred, VERB, O, ), (...  \n",
       "679  [(Swimming, NOUN, O, ), (showdown, NOUN, O, ),...  \n",
       "629  [(Millions, NOUN, B, CARDINAL), (Wait, VERB, O...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data['tokens_filtered'] = small_data['tokens'].apply(filter_tokens)\n",
    "small_data['text_filtered'] = small_data['tokens_filtered'].apply(filter_text)\n",
    "small_data['text_embeddings'] = small_data['tokens_filtered'].apply(text_embeddings)\n",
    "small_data['text_ner'] = small_data['tokens_filtered'].apply(text_ner)\n",
    "small_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gdrp7\\AppData\\Local\\Temp\\ipykernel_9564\\237457956.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  small_data['entity_dict'] = small_data['text_filtered'].apply(process_entities)\n"
     ]
    }
   ],
   "source": [
    "def process_entities(text_filtered):\n",
    "    entity_dict = {}\n",
    "    doc = nlp(text_filtered)\n",
    "    for ent in doc.ents:\n",
    "        if str(ent) not in entity_dict:\n",
    "            entity_dict[ent.lemma_] = (ent.root.pos_, ent.label_)\n",
    "\n",
    "    # non_entity_strings = [token for token in doc \n",
    "    #                     if token.text not in entity_dict \n",
    "    #                     and token.ent_iob_ == \"O\"\n",
    "    #                     and token.pos_ != 'SPACE']\n",
    "    # entity_dict.update({token.lemma_: (token.pos_, None) for token in non_entity_strings})\n",
    "\n",
    "    return entity_dict\n",
    "\n",
    "small_data['entity_dict'] = small_data['text_filtered'].apply(process_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_count</th>\n",
       "      <th>tokens_filtered</th>\n",
       "      <th>text_filtered</th>\n",
       "      <th>text_embeddings</th>\n",
       "      <th>text_ner</th>\n",
       "      <th>entity_dict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>AP: Group Discovers John the Baptist Cave KIBB...</td>\n",
       "      <td>0</td>\n",
       "      <td>(AP, :, Group, Discovers, John, the, Baptist, ...</td>\n",
       "      <td>90</td>\n",
       "      <td>[AP, Group, Discovers, John, Baptist, Cave, KI...</td>\n",
       "      <td>ap group discovers john baptist cave kibbutz t...</td>\n",
       "      <td>[[0.73506, -0.13789, 9.1416, 11.004, 3.2473, 0...</td>\n",
       "      <td>[(AP, NOUN, B, ORG), (Group, PROPN, B, ORG), (...</td>\n",
       "      <td>{'ap': ('PROPN', 'ORG'), 'john baptist': ('PRO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>Letter, suspicious powder spark anthrax scare ...</td>\n",
       "      <td>0</td>\n",
       "      <td>(Letter, ,, suspicious, powder, spark, anthrax...</td>\n",
       "      <td>67</td>\n",
       "      <td>[Letter, suspicious, powder, spark, anthrax, s...</td>\n",
       "      <td>letter suspicious powder spark anthrax scare u...</td>\n",
       "      <td>[[-2.215, -1.9555, 2.0093, 0.67661, 2.7137, -0...</td>\n",
       "      <td>[(Letter, NOUN, O, ), (suspicious, ADJ, O, ), ...</td>\n",
       "      <td>{'u.s.': ('PROPN', 'GPE'), 'malaysia': ('PROPN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>3D holograms to crack forgeries A 3D hologram ...</td>\n",
       "      <td>3</td>\n",
       "      <td>(3D, holograms, to, crack, forgeries, A, 3D, h...</td>\n",
       "      <td>21</td>\n",
       "      <td>[3D, holograms, crack, forgeries, 3D, hologram...</td>\n",
       "      <td>3d hologram crack forgery 3d hologram techniqu...</td>\n",
       "      <td>[[-6.4571, -2.9551, -6.4594, -1.8931, -2.8373,...</td>\n",
       "      <td>[(3D, ADJ, O, ), (holograms, NOUN, O, ), (crac...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Experts Downplay Texas Shark Attacks (AP) AP -...</td>\n",
       "      <td>3</td>\n",
       "      <td>(Experts, Downplay, Texas, Shark, Attacks, (, ...</td>\n",
       "      <td>51</td>\n",
       "      <td>[Experts, Downplay, Texas, Shark, Attacks, AP,...</td>\n",
       "      <td>expert downplay texas shark attacks ap ap shar...</td>\n",
       "      <td>[[2.9159, -0.21193, 0.47963, 2.3368, -0.6856, ...</td>\n",
       "      <td>[(Experts, NOUN, O, ), (Downplay, PROPN, O, ),...</td>\n",
       "      <td>{'texas': ('PROPN', 'GPE'), 'ap ap': ('PROPN',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>Mass. Republicans Eye Kerry's Senate Seat (AP)...</td>\n",
       "      <td>0</td>\n",
       "      <td>(Mass., Republicans, Eye, Kerry, 's, Senate, S...</td>\n",
       "      <td>67</td>\n",
       "      <td>[Mass., Republicans, Eye, Kerry, Senate, Seat,...</td>\n",
       "      <td>mass. republicans eye kerry senate seat ap ap ...</td>\n",
       "      <td>[[-0.3604, -0.72662, 1.7799, 5.8378, 1.6046, -...</td>\n",
       "      <td>[(Mass., PROPN, B, GPE), (Republicans, PROPN, ...</td>\n",
       "      <td>{'republicans': ('PROPN', 'NORP'), 'kerry sena...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label  \\\n",
       "954  AP: Group Discovers John the Baptist Cave KIBB...      0   \n",
       "869  Letter, suspicious powder spark anthrax scare ...      0   \n",
       "199  3D holograms to crack forgeries A 3D hologram ...      3   \n",
       "154  Experts Downplay Texas Shark Attacks (AP) AP -...      3   \n",
       "783  Mass. Republicans Eye Kerry's Senate Seat (AP)...      0   \n",
       "\n",
       "                                                tokens  tokens_count  \\\n",
       "954  (AP, :, Group, Discovers, John, the, Baptist, ...            90   \n",
       "869  (Letter, ,, suspicious, powder, spark, anthrax...            67   \n",
       "199  (3D, holograms, to, crack, forgeries, A, 3D, h...            21   \n",
       "154  (Experts, Downplay, Texas, Shark, Attacks, (, ...            51   \n",
       "783  (Mass., Republicans, Eye, Kerry, 's, Senate, S...            67   \n",
       "\n",
       "                                       tokens_filtered  \\\n",
       "954  [AP, Group, Discovers, John, Baptist, Cave, KI...   \n",
       "869  [Letter, suspicious, powder, spark, anthrax, s...   \n",
       "199  [3D, holograms, crack, forgeries, 3D, hologram...   \n",
       "154  [Experts, Downplay, Texas, Shark, Attacks, AP,...   \n",
       "783  [Mass., Republicans, Eye, Kerry, Senate, Seat,...   \n",
       "\n",
       "                                         text_filtered  \\\n",
       "954  ap group discovers john baptist cave kibbutz t...   \n",
       "869  letter suspicious powder spark anthrax scare u...   \n",
       "199  3d hologram crack forgery 3d hologram techniqu...   \n",
       "154  expert downplay texas shark attacks ap ap shar...   \n",
       "783  mass. republicans eye kerry senate seat ap ap ...   \n",
       "\n",
       "                                       text_embeddings  \\\n",
       "954  [[0.73506, -0.13789, 9.1416, 11.004, 3.2473, 0...   \n",
       "869  [[-2.215, -1.9555, 2.0093, 0.67661, 2.7137, -0...   \n",
       "199  [[-6.4571, -2.9551, -6.4594, -1.8931, -2.8373,...   \n",
       "154  [[2.9159, -0.21193, 0.47963, 2.3368, -0.6856, ...   \n",
       "783  [[-0.3604, -0.72662, 1.7799, 5.8378, 1.6046, -...   \n",
       "\n",
       "                                              text_ner  \\\n",
       "954  [(AP, NOUN, B, ORG), (Group, PROPN, B, ORG), (...   \n",
       "869  [(Letter, NOUN, O, ), (suspicious, ADJ, O, ), ...   \n",
       "199  [(3D, ADJ, O, ), (holograms, NOUN, O, ), (crac...   \n",
       "154  [(Experts, NOUN, O, ), (Downplay, PROPN, O, ),...   \n",
       "783  [(Mass., PROPN, B, GPE), (Republicans, PROPN, ...   \n",
       "\n",
       "                                           entity_dict  \n",
       "954  {'ap': ('PROPN', 'ORG'), 'john baptist': ('PRO...  \n",
       "869  {'u.s.': ('PROPN', 'GPE'), 'malaysia': ('PROPN...  \n",
       "199                                                 {}  \n",
       "154  {'texas': ('PROPN', 'GPE'), 'ap ap': ('PROPN',...  \n",
       "783  {'republicans': ('PROPN', 'NORP'), 'kerry sena...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the filtered text and tokens in its own dataframe and save to csv\n",
    "small_data_filtered = small_data[['text_filtered', 'label',]]\n",
    "small_data_filtered.to_csv('datasets/small_data_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# split the data into training and testing sets\n",
    "train, test = train_test_split(small_data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_features = []\n",
    "train_features_embeddes = []\n",
    "train_features_text_and_tokens = []\n",
    "train_features_entities = []\n",
    "train_features_no_er = []\n",
    "train_labels = []\n",
    "test_features = []\n",
    "test_features_embeddes = []\n",
    "test_features_text_and_tokens = []\n",
    "test_features_entities = []\n",
    "test_features_no_er = []\n",
    "test_labels = []\n",
    "\n",
    "for index, row in train.iterrows():\n",
    "    entity_dict = row['entity_dict']\n",
    "    word_embeddings = np.array(row['text_embeddings'])  # Convert list to numpy array\n",
    "    word_embeddings_doc = np.mean(word_embeddings, axis=0)  # Average the word embeddings\n",
    "    text_filtered = row['text_filtered']\n",
    "    tokens_filtered = row['tokens_filtered']\n",
    "    label = row['label']\n",
    "    \n",
    "    # All features\n",
    "    features = {\n",
    "        'entity_dict': str(entity_dict),  # Convert dictionary to string\n",
    "        'word_embeddings': [str(embedding) for embedding in word_embeddings.tolist()],  # Convert numpy array to list of strings\n",
    "        'text_filtered': str(text_filtered),  # Convert token to string\n",
    "        'tokens_filtered': [str(token) for token in tokens_filtered]  # Convert tokens to strings\n",
    "    }\n",
    "    train_features.append(features)\n",
    "    \n",
    "    # Only embeddings\n",
    "    features_embedds = {\n",
    "        'word_embeddings': [str(embedding) for embedding in word_embeddings.tolist()],  # Convert numpy array to list of strings\n",
    "    } \n",
    "    train_features_embeddes.append(features_embedds)\n",
    "    \n",
    "    # Only text and tokens\n",
    "    features_text_and_tokens = {\n",
    "        'text_filtered': str(text_filtered),  # Convert token to string\n",
    "        'tokens_filtered': [str(token) for token in tokens_filtered]  # Convert tokens to strings\n",
    "    }\n",
    "    train_features_text_and_tokens.append(features_text_and_tokens)\n",
    "    \n",
    "    # Only entities\n",
    "    features_entities = {\n",
    "        'entity_dict': str(entity_dict),  # Convert dictionary to string\n",
    "    }\n",
    "    train_features_entities.append(features_entities)\n",
    "    \n",
    "    # No NER\n",
    "    features_no_ner = {\n",
    "        'word_embeddings': [str(embedding) for embedding in word_embeddings.tolist()],  # Convert numpy array to list of strings\n",
    "        'text_filtered': str(text_filtered),  # Convert token to string\n",
    "        'tokens_filtered': [str(token) for token in tokens_filtered]  # Convert tokens to strings\n",
    "    } \n",
    "    train_features_no_er.append(features_no_ner)\n",
    "    \n",
    "    train_labels.append(label)\n",
    "    \n",
    "for index, row in test.iterrows():\n",
    "    entity_dict = row['entity_dict']\n",
    "    word_embeddings = np.array(row['text_embeddings'])  # Convert list to numpy array\n",
    "    word_embeddings_doc = np.mean(word_embeddings, axis=0)  # Average the word embeddings\n",
    "    text_filtered = row['text_filtered']\n",
    "    tokens_filtered = row['tokens_filtered']\n",
    "    label = row['label']\n",
    "    \n",
    "    features_test = {\n",
    "        'entity_dict': str(entity_dict),  # Convert dictionary to string\n",
    "        'word_embeddings': [str(embedding) for embedding in word_embeddings.tolist()],  # Convert numpy array to list of strings\n",
    "        'text_filtered': str(text_filtered),  # Convert token to string\n",
    "        'tokens_filtered': [str(token) for token in tokens_filtered]  # Convert tokens to strings\n",
    "    }\n",
    "    test_features.append(features_test)\n",
    "    \n",
    "    # Only embeddings\n",
    "    features_embedds = {\n",
    "        'word_embeddings': [str(embedding) for embedding in word_embeddings.tolist()],  # Convert numpy array to list of strings\n",
    "    } \n",
    "    test_features_embeddes.append(features_embedds)\n",
    "    \n",
    "    # Only text and tokens\n",
    "    features_text_and_tokens = {\n",
    "        'text_filtered': str(text_filtered),  # Convert token to string\n",
    "        'tokens_filtered': [str(token) for token in tokens_filtered]  # Convert tokens to strings\n",
    "    }\n",
    "    test_features_text_and_tokens.append(features_text_and_tokens)\n",
    "    \n",
    "    # Only entities\n",
    "    features_entities = {\n",
    "        'entity_dict': str(entity_dict),  # Convert dictionary to string\n",
    "    }\n",
    "    test_features_entities.append(features_entities)\n",
    "    \n",
    "    # No NER\n",
    "    features_no_ner = {\n",
    "        'word_embeddings': [str(embedding) for embedding in word_embeddings.tolist()],  # Convert numpy array to list of strings\n",
    "        'text_filtered': str(text_filtered),  # Convert token to string\n",
    "        'tokens_filtered': [str(token) for token in tokens_filtered]  # Convert tokens to strings\n",
    "    } \n",
    "    test_features_no_er.append(features_no_ner)\n",
    "    test_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def modelFit(model, param_grid, train_features, test_features):\n",
    "    pipeline = Pipeline([\n",
    "        ('vectorizer', DictVectorizer()),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "\n",
    "    # Add max_iter to param_grid if the model supports it\n",
    "    try:\n",
    "        if hasattr(model, 'max_iter'):\n",
    "            param_grid['classifier__max_iter'] = [6000]  # or whatever values you want\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred when setting max_iter: {e}\")\n",
    "\n",
    "\n",
    "    # Create the GridSearchCV object\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=10, scoring='accuracy')\n",
    "\n",
    "    # Fit the GridSearchCV object\n",
    "    grid_search.fit(train_features, train_labels)\n",
    "\n",
    "    # Get the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Predict the test data\n",
    "    preds = best_model.predict(test_features)\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    accuracy = accuracy_score(test_labels, preds)\n",
    "    f1 = f1_score(test_labels, preds, average='weighted')\n",
    "    precision = precision_score(test_labels, preds, average='weighted')\n",
    "    recall = recall_score(test_labels, preds, average='weighted')\n",
    "\n",
    "    results = {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'confusion_matrix': confusion_matrix(test_labels, preds),\n",
    "        'best_params': grid_search.best_params_\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## NB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.805, 'f1': 0.8008366169234516, 'precision': 0.8027972984394132, 'recall': 0.805, 'confusion_matrix': array([[30,  5,  0,  4],\n",
      "       [ 0, 26,  1,  0],\n",
      "       [ 0,  0, 22, 15],\n",
      "       [ 7,  1,  6, 83]], dtype=int64), 'best_params': {}}\n"
     ]
    }
   ],
   "source": [
    "results = modelFit(MultinomialNB(), {}, train_features, test_features)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only text and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.78, 'f1': 0.7753124338624338, 'precision': 0.7794217341078741, 'recall': 0.78, 'confusion_matrix': array([[31,  5,  0,  3],\n",
      "       [ 2, 23,  1,  1],\n",
      "       [ 0,  0, 20, 17],\n",
      "       [ 9,  1,  5, 82]], dtype=int64), 'best_params': {}}\n"
     ]
    }
   ],
   "source": [
    "results = modelFit(MultinomialNB(), {}, train_features_text_and_tokens, test_features_text_and_tokens)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only entities and PoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.48, 'f1': 0.3145945945945946, 'precision': 0.23396984924623115, 'recall': 0.48, 'confusion_matrix': array([[ 0,  0,  0, 39],\n",
      "       [ 0,  0,  0, 27],\n",
      "       [ 0,  0,  0, 37],\n",
      "       [ 1,  0,  0, 96]], dtype=int64), 'best_params': {}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gdrp7\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "results = modelFit(MultinomialNB(), {}, train_features_entities, test_features_entities)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Dense Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.825, 'f1': 0.8210554285168502, 'precision': 0.8246121873815838, 'recall': 0.825, 'confusion_matrix': array([[30,  5,  0,  4],\n",
      "       [ 0, 26,  1,  0],\n",
      "       [ 0,  0, 23, 14],\n",
      "       [ 5,  1,  5, 86]], dtype=int64), 'best_params': {}}\n"
     ]
    }
   ],
   "source": [
    "results = modelFit(MultinomialNB(), {}, train_features_embeddes, test_features_embeddes)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.805, 'f1': 0.8008366169234516, 'precision': 0.8027972984394132, 'recall': 0.805, 'confusion_matrix': array([[30,  5,  0,  4],\n",
      "       [ 0, 26,  1,  0],\n",
      "       [ 0,  0, 22, 15],\n",
      "       [ 7,  1,  6, 83]], dtype=int64), 'best_params': {}}\n"
     ]
    }
   ],
   "source": [
    "results = modelFit(MultinomialNB(), {}, train_features_no_er, test_features_no_er)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gdrp7\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\gdrp7\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\gdrp7\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\gdrp7\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\gdrp7\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\gdrp7\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "results = modelFit(LogisticRegression(), {\n",
    "    'classifier__C': [0.1, 1.0, 10.0],\n",
    "    'classifier__penalty': ['l1', 'l2'],\n",
    "    'classifier__solver': ['liblinear', 'saga']\n",
    "}, train_features, test_features)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only text and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.79, 'f1': 0.7817663323393873, 'precision': 0.796355376344086, 'recall': 0.79, 'confusion_matrix': array([[27,  3,  2,  7],\n",
      "       [ 1, 21,  1,  4],\n",
      "       [ 0,  0, 19, 18],\n",
      "       [ 3,  0,  3, 91]]), 'best_params': {'classifier__C': 1.0, 'classifier__penalty': 'l2', 'classifier__solver': 'saga'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "results = modelFit(LogisticRegression(), {\n",
    "    'classifier__C': [0.1, 1.0, 10.0],\n",
    "    'classifier__penalty': ['l1', 'l2'],\n",
    "    'classifier__solver': ['liblinear', 'saga']\n",
    "}, train_features_text_and_tokens, test_features_text_and_tokens)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only entities and PoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.5, 'f1': 0.3550016705646509, 'precision': 0.4237692307692308, 'recall': 0.5, 'confusion_matrix': array([[ 0,  0,  0, 39],\n",
      "       [ 0,  0,  0, 27],\n",
      "       [ 0,  0,  4, 33],\n",
      "       [ 1,  0,  0, 96]]), 'best_params': {'classifier__C': 1.0, 'classifier__penalty': 'l2', 'classifier__solver': 'saga'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "results = modelFit(LogisticRegression(), {\n",
    "    'classifier__C': [0.1, 1.0, 10.0],\n",
    "    'classifier__penalty': ['l1', 'l2'],\n",
    "    'classifier__solver': ['liblinear', 'saga']\n",
    "}, train_features_entities, test_features_entities)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only dense embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/lucas/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodelFit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLogisticRegression\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclassifier__C\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclassifier__penalty\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ml1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ml2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclassifier__solver\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mliblinear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msaga\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_features_embeddes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_features_embeddes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n",
      "Cell \u001b[0;32mIn[16], line 13\u001b[0m, in \u001b[0;36mmodelFit\u001b[0;34m(model, param_grid, train_features, test_features)\u001b[0m\n\u001b[1;32m     10\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(pipeline, param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Fit the GridSearchCV object\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Get the best model\u001b[39;00m\n\u001b[1;32m     16\u001b[0m best_model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/model_selection/_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1422\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/model_selection/_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    842\u001b[0m         )\n\u001b[1;32m    843\u001b[0m     )\n\u001b[0;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/utils/parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:729\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    727\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    728\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 729\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    733\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/pipeline.py:427\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    426\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 427\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_last_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1303\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1300\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1301\u001b[0m     n_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1303\u001b[0m fold_coefs_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mC_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1328\u001b[0m fold_coefs_, _, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mfold_coefs_)\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(n_iter_, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/utils/parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:533\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[1;32m    530\u001b[0m         alpha \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m C) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m l1_ratio)\n\u001b[1;32m    531\u001b[0m         beta \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m C) \u001b[38;5;241m*\u001b[39m l1_ratio\n\u001b[0;32m--> 533\u001b[0m     w0, n_iter_i, warm_start_sag \u001b[38;5;241m=\u001b[39m \u001b[43msag_solver\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarm_start_sag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_saga\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaga\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    552\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msolver must be one of \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlbfgs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    553\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewton-cg\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msag\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m}, got \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m solver\n\u001b[1;32m    554\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:325\u001b[0m, in \u001b[0;36msag_solver\u001b[0;34m(X, y, sample_weight, loss, alpha, beta, max_iter, tol, verbose, random_state, check_input, max_squared_sum, warm_start_mem, is_saga)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mZeroDivisionError\u001b[39;00m(\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent sag implementation does not handle \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe case step_size * alpha_scaled == 1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[1;32m    324\u001b[0m sag \u001b[38;5;241m=\u001b[39m sag64 \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat64 \u001b[38;5;28;01melse\u001b[39;00m sag32\n\u001b[0;32m--> 325\u001b[0m num_seen, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[43msag\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintercept_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha_scaled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta_scaled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43msum_gradient_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_memory_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseen_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_seen_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintercept_sum_gradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintercept_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_saga\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_iter_ \u001b[38;5;241m==\u001b[39m max_iter:\n\u001b[1;32m    350\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    351\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe max_iter was reached which means the coef_ did not converge\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    352\u001b[0m         ConvergenceWarning,\n\u001b[1;32m    353\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = modelFit(LogisticRegression(), {\n",
    "    'classifier__C': [0.1, 1.0, 10.0],\n",
    "    'classifier__penalty': ['l1', 'l2'],\n",
    "    'classifier__solver': ['liblinear', 'saga']\n",
    "}, train_features_embeddes, test_features_embeddes)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(LogisticRegression(), {\n",
    "    'classifier__C': [0.1, 1.0, 10.0],\n",
    "    'classifier__penalty': ['l1', 'l2'],\n",
    "    'classifier__solver': ['liblinear', 'saga']\n",
    "}, train_features_no_er, test_features_no_er)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## DTC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "results = modelFit(DecisionTreeClassifier(), {\n",
    "        'classifier__max_depth': [None, 5, 10, 15],\n",
    "        'classifier__min_samples_split': [2, 5, 10]\n",
    "    }, train_features, test_features)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only text and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(DecisionTreeClassifier(), {\n",
    "        'classifier__max_depth': [None, 5, 10, 15],\n",
    "        'classifier__min_samples_split': [2, 5, 10]\n",
    "    }, train_features_text_and_tokens, test_features_text_and_tokens)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only entites and PoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(DecisionTreeClassifier(), {\n",
    "        'classifier__max_depth': [None, 5, 10, 15],\n",
    "        'classifier__min_samples_split': [2, 5, 10]\n",
    "    }, train_features_entities, test_features_entities)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only dense embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(DecisionTreeClassifier(), {\n",
    "        'classifier__max_depth': [None, 5, 10, 15],\n",
    "        'classifier__min_samples_split': [2, 5, 10]\n",
    "    }, train_features_embeddes, test_features_embeddes)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(DecisionTreeClassifier(), {\n",
    "        'classifier__max_depth': [None, 5, 10, 15],\n",
    "        'classifier__min_samples_split': [2, 5, 10]\n",
    "    }, train_features_no_er, test_features_no_er)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklean.ensemble import RandomForestClassifier\n",
    "\n",
    "results = modelFit(RandomForestClassifier(), {\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__max_depth': [None, 5, 10, 15],\n",
    "        'classifier__min_samples_split': [5, 10, 20, 30]\n",
    "    }, train_features, test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only text and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(RandomForestClassifier(), {\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__max_depth': [None, 5, 10, 15],\n",
    "        'classifier__min_samples_split': [5, 10, 20, 30]\n",
    "    }, train_features_text_and_tokens, test_features_text_and_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only entities and PoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(RandomForestClassifier(), {\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__max_depth': [None, 5, 10, 15],\n",
    "        'classifier__min_samples_split': [5, 10, 20, 30]\n",
    "    }, train_features_entities, test_features_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only dense embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(RandomForestClassifier(), {\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__max_depth': [None, 5, 10, 15],\n",
    "        'classifier__min_samples_split': [5, 10, 20, 30]\n",
    "    }, train_features_embeddes, test_features_embeddes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(RandomForestClassifier(), {\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__max_depth': [None, 5, 10, 15],\n",
    "        'classifier__min_samples_split': [5, 10, 20, 30]\n",
    "    }, train_features_no_er, test_features_no_er)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "results = modelFit(SVC(), {\n",
    "        'classifier__C': [0.1, 1.0, 10.0],\n",
    "        'classifier__kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "    }, train_features, test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only text and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(SVC(), {\n",
    "        'classifier__C': [0.1, 1.0, 10.0],\n",
    "        'classifier__kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "    }, train_features_text_and_tokens, test_features_text_and_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only entities and PoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(SVC(), {\n",
    "        'classifier__C': [0.1, 1.0, 10.0],\n",
    "        'classifier__kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "    }, train_features_entities, test_features_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only dense embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(SVC(), {\n",
    "        'classifier__C': [0.1, 1.0, 10.0],\n",
    "        'classifier__kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "    }, train_features_embeddes, test_features_embeddes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(SVC(), {\n",
    "        'classifier__C': [0.1, 1.0, 10.0],\n",
    "        'classifier__kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "    }, train_features_no_er, test_features_no_er)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "results = modelFit(XGBClassifier(), {\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__max_depth': [3, 5, 7, 9]\n",
    "    }, train_features, test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only text and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(XGBClassifier(), {\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__max_depth': [3, 5, 7, 9]\n",
    "    }, train_features_text_and_tokens, test_features_text_and_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only entities and PoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(XGBClassifier(), {\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__max_depth': [3, 5, 7, 9]\n",
    "    }, train_features_entities, test_features_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only dense embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(XGBClassifier(), {\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__max_depth': [3, 5, 7, 9]\n",
    "    }, train_features_embeddes, test_features_embeddes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(XGBClassifier(), {\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__max_depth': [3, 5, 7, 9]\n",
    "    }, train_features_no_er, test_features_no_er)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "results = modelFit(GradientBoostingClassifier(), {\n",
    "    'classifier__n_estimators': [50, 100, 200],\n",
    "    'classifier__learning_rate': [0.01, 0.1, 0.5],\n",
    "    'classifier__max_depth': [3, 5, 7]\n",
    "}, train_features, test_features)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only text and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(GradientBoostingClassifier(), {\n",
    "    'classifier__n_estimators': [50, 100, 200],\n",
    "    'classifier__learning_rate': [0.01, 0.1, 0.5],\n",
    "    'classifier__max_depth': [3, 5, 7]\n",
    "}, train_features_text_and_tokens, test_features_text_and_tokens)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only entities and PoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(GradientBoostingClassifier(), {\n",
    "    'classifier__n_estimators': [50, 100, 200],\n",
    "    'classifier__learning_rate': [0.01, 0.1, 0.5],\n",
    "    'classifier__max_depth': [3, 5, 7]\n",
    "}, train_features_entities, test_features_entities)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Dense Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(GradientBoostingClassifier(), {\n",
    "    'classifier__n_estimators': [50, 100, 200],\n",
    "    'classifier__learning_rate': [0.01, 0.1, 0.5],\n",
    "    'classifier__max_depth': [3, 5, 7]\n",
    "}, train_features_embeddes, test_features_embeddes)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(GradientBoostingClassifier(), {\n",
    "    'classifier__n_estimators': [50, 100, 200],\n",
    "    'classifier__learning_rate': [0.01, 0.1, 0.5],\n",
    "    'classifier__max_depth': [3, 5, 7]\n",
    "}, train_features_no_er, test_features_no_er)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "results = modelFit(KNeighborsClassifier(), {\n",
    "    'classifier__n_neighbors': [3, 5, 7, 10],\n",
    "    'classifier__weights': ['uniform', 'distance'],\n",
    "    'classifier__p': [1, 2]  # 1 for Manhattan distance, 2 for Euclidean distance\n",
    "}, train_features, test_features)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only text and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(KNeighborsClassifier(), {\n",
    "    'classifier__n_neighbors': [3, 5, 7, 10],\n",
    "    'classifier__weights': ['uniform', 'distance'],\n",
    "    'classifier__p': [1, 2]  # 1 for Manhattan distance, 2 for Euclidean distance\n",
    "}, train_features_text_and_tokens, test_features_text_and_tokens)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only entities and PoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(KNeighborsClassifier(), {\n",
    "    'classifier__n_neighbors': [3, 5, 7, 10],\n",
    "    'classifier__weights': ['uniform', 'distance'],\n",
    "    'classifier__p': [1, 2]  # 1 for Manhattan distance, 2 for Euclidean distance\n",
    "}, train_features_entities, test_features_entities)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Dense Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(KNeighborsClassifier(), {\n",
    "    'classifier__n_neighbors': [3, 5, 7, 10],\n",
    "    'classifier__weights': ['uniform', 'distance'],\n",
    "    'classifier__p': [1, 2]  # 1 for Manhattan distance, 2 for Euclidean distance\n",
    "}, train_features_embeddes, test_features_embeddes)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(KNeighborsClassifier(), {\n",
    "    'classifier__n_neighbors': [3, 5, 7, 10],\n",
    "    'classifier__weights': ['uniform', 'distance'],\n",
    "    'classifier__p': [1, 2]  # 1 for Manhattan distance, 2 for Euclidean distance\n",
    "}, train_features_no_er, test_features_no_er)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "results = modelFit(MLPClassifier(), {\n",
    "    'classifier__hidden_layer_sizes': [(100,), (50, 50), (50, 100, 50)],\n",
    "    'classifier__activation': ['logistic', 'relu'],\n",
    "    'classifier__solver': ['adam', 'sgd'],\n",
    "    'classifier__alpha': [0.0001, 0.001, 0.01],\n",
    "    'classifier__learning_rate': ['constant', 'adaptive']\n",
    "}, train_features, test_features)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only text and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(MLPClassifier(), {\n",
    "    'classifier__hidden_layer_sizes': [(100,), (50, 50), (50, 100, 50)],\n",
    "    'classifier__activation': ['logistic', 'relu'],\n",
    "    'classifier__solver': ['adam', 'sgd'],\n",
    "    'classifier__alpha': [0.0001, 0.001, 0.01],\n",
    "    'classifier__learning_rate': ['constant', 'adaptive']\n",
    "}, train_features_text_and_tokens, test_features_text_and_tokens)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only entities and PoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(MLPClassifier(), {\n",
    "    'classifier__hidden_layer_sizes': [(100,), (50, 50), (50, 100, 50)],\n",
    "    'classifier__activation': ['logistic', 'relu'],\n",
    "    'classifier__solver': ['adam', 'sgd'],\n",
    "    'classifier__alpha': [0.0001, 0.001, 0.01],\n",
    "    'classifier__learning_rate': ['constant', 'adaptive']\n",
    "}, train_features_entities, test_features_entities)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Dense Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(MLPClassifier(), {\n",
    "    'classifier__hidden_layer_sizes': [(100,), (50, 50), (50, 100, 50)],\n",
    "    'classifier__activation': ['logistic', 'relu'],\n",
    "    'classifier__solver': ['adam', 'sgd'],\n",
    "    'classifier__alpha': [0.0001, 0.001, 0.01],\n",
    "    'classifier__learning_rate': ['constant', 'adaptive']\n",
    "}, train_features_embeddes, test_features_embeddes)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(MLPClassifier(), {\n",
    "    'classifier__hidden_layer_sizes': [(100,), (50, 50), (50, 100, 50)],\n",
    "    'classifier__activation': ['logistic', 'relu'],\n",
    "    'classifier__solver': ['adam', 'sgd'],\n",
    "    'classifier__alpha': [0.0001, 0.001, 0.01],\n",
    "    'classifier__learning_rate': ['constant', 'adaptive']\n",
    "}, train_features_no_er, test_features_no_er)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
