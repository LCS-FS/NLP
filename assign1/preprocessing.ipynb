{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3050</th>\n",
       "      <td>Stocks Are Up Despite Rising Oil Prices NEW YO...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7642</th>\n",
       "      <td>Mexico Arrests Major Drug Trafficker (AP) AP -...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52527</th>\n",
       "      <td>US Files Grievance Over Airbus With WTO The Bu...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103697</th>\n",
       "      <td>Steelers defense lives up to ranking The Washi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24301</th>\n",
       "      <td>9/11: NY port authority sues Saudi Arabia The ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label\n",
       "3050    Stocks Are Up Despite Rising Oil Prices NEW YO...      0\n",
       "7642    Mexico Arrests Major Drug Trafficker (AP) AP -...      0\n",
       "52527   US Files Grievance Over Airbus With WTO The Bu...      2\n",
       "103697  Steelers defense lives up to ranking The Washi...      1\n",
       "24301   9/11: NY port authority sues Saudi Arabia The ...      0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('datasets/training_data.csv')\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = ' '.join(data['text'])\n",
    "# print(corpus[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not perfect, there's instances like '(Reuters)' that are not removed\n",
    "def remove_reuters(text):\n",
    "    pattern = r'\\((\\w+)\\) \\1+'\n",
    "    return re.sub(pattern, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(text):\n",
    "    # Example\n",
    "    # &lt;A HREF=\"\"http://www.investor.reuters.com/FullQuote.aspx?ticker=BNNY.OB target=/stocks/quickinfo/fullquote\"\"&gt;BNNY.OB&lt;/A&gt;\n",
    "    pattern = r'&lt;A HREF=.*?&gt;(.*?)&lt;/A&gt;'\n",
    "    return re.sub(pattern, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    decoded_text = html.unescape(text)\n",
    "    pattern = r'<.*?>'\n",
    "    return re.sub(pattern, '', decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_quotes(text):\n",
    "    pattern = r'quot;'\n",
    "    return re.sub(pattern, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Somewhere between gleam and gloom President Bush has been saying that the US economy has ''turned the corner.  Democratic presidential candidate Senator John F. Kerry, in the wake of this month's poor jobs report, quipped that it was more like a U-turn.\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'] = data['text'].apply(remove_links)\n",
    "data['text'] = data['text'].apply(remove_reuters)\n",
    "data['text'] = data['text'].apply(remove_html_tags)\n",
    "data['text'] = data['text'].apply(remove_quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wall St. Bears Claw Back Into the Black  - Sho...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace  - P...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook  - Soari...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Wall St. Bears Claw Back Into the Black  - Sho...      2\n",
       "1  Carlyle Looks Toward Commercial Aerospace  - P...      2\n",
       "2  Oil and Economy Cloud Stocks' Outlook  - Soari...      2\n",
       "3  Iraq Halts Oil Exports from Main Southern Pipe...      2\n",
       "4  Oil prices soar to all-time record, posing new...      2"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Customize pipeline\n",
    "# nlp.remove_pipe('tok2vec')\n",
    "# nlp.remove_pipe('tagger')\n",
    "# nlp.remove_pipe('parser')\n",
    "# nlp.remove_pipe('attribute_ruler')\n",
    "# nlp.remove_pipe('lemmatizer')\n",
    "# nlp.remove_pipe('ner')\n",
    "\n",
    "# nlp.enable_pipe('senter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21015</th>\n",
       "      <td>A break with tradition If you think of your bu...</td>\n",
       "      <td>2</td>\n",
       "      <td>(A, break, with, tradition, If, you, think, of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29258</th>\n",
       "      <td>UCB up after deal to sell unit to Cytec for \\$...</td>\n",
       "      <td>2</td>\n",
       "      <td>(UCB, up, after, deal, to, sell, unit, to, Cyt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16959</th>\n",
       "      <td>Yankees low key after clinching post-season sp...</td>\n",
       "      <td>1</td>\n",
       "      <td>(Yankees, low, key, after, clinching, post, -,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22759</th>\n",
       "      <td>Future looks bleak for Yukos HAMISH ROBERTSON:...</td>\n",
       "      <td>2</td>\n",
       "      <td>(Future, looks, bleak, for, Yukos, HAMISH, ROB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12916</th>\n",
       "      <td>Heeling power During every break in play, ever...</td>\n",
       "      <td>1</td>\n",
       "      <td>(Heeling, power, During, every, break, in, pla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label  \\\n",
       "21015  A break with tradition If you think of your bu...      2   \n",
       "29258  UCB up after deal to sell unit to Cytec for \\$...      2   \n",
       "16959  Yankees low key after clinching post-season sp...      1   \n",
       "22759  Future looks bleak for Yukos HAMISH ROBERTSON:...      2   \n",
       "12916  Heeling power During every break in play, ever...      1   \n",
       "\n",
       "                                                  tokens  \n",
       "21015  (A, break, with, tradition, If, you, think, of...  \n",
       "29258  (UCB, up, after, deal, to, sell, unit, to, Cyt...  \n",
       "16959  (Yankees, low, key, after, clinching, post, -,...  \n",
       "22759  (Future, looks, bleak, for, Yukos, HAMISH, ROB...  \n",
       "12916  (Heeling, power, During, every, break, in, pla...  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: process all data\n",
    "#small_data = data.sample(30000)\n",
    "\n",
    "#get 30000 entries with the same distribution of the original data\n",
    "small_data = data.groupby('label').apply(lambda x: x.sample(10000)).reset_index(drop=True)\n",
    "\n",
    "#small_data = data.copy()\n",
    "small_data['tokens'] = small_data['text'].apply(nlp)\n",
    "small_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Wall St. Bears Claw Back Into the Black  - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data.iloc[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    40000.000000\n",
       "mean        44.057950\n",
       "std         12.376145\n",
       "min         12.000000\n",
       "25%         36.000000\n",
       "50%         43.000000\n",
       "75%         50.000000\n",
       "max        203.000000\n",
       "Name: tokens_count, dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data['tokens_count'] = small_data['tokens'].apply(len)\n",
    "small_data['tokens_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tokens(tokens):\n",
    "    tokens_without_punctuation = [token for token in tokens if not token.is_punct]\n",
    "    tokens_without_space = [token for token in tokens_without_punctuation if not token.is_space]\n",
    "    tokens_without_stopwords = [token for token in tokens_without_space if not token.is_stop]\n",
    "    return tokens_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text(text):\n",
    "    tokens_lemmatized = [token.lemma_ for token in text]\n",
    "    tokens_lower = [token.lower() for token in tokens_lemmatized]\n",
    "    return ' '.join(tokens_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_embeddings(text):\n",
    "    token_embeddings = [token.vector for token in text]\n",
    "    return token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_ner(text):\n",
    "    return [(token, token.pos_, token.ent_iob_, token.ent_type_) for token in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_count</th>\n",
       "      <th>tokens_filtered</th>\n",
       "      <th>text_filtered</th>\n",
       "      <th>text_embeddings</th>\n",
       "      <th>text_ner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9305</th>\n",
       "      <td>Greenspan: Oil Price Surge Not Big Issue WASHI...</td>\n",
       "      <td>0</td>\n",
       "      <td>(Greenspan, :, Oil, Price, Surge, Not, Big, Is...</td>\n",
       "      <td>82</td>\n",
       "      <td>[Greenspan, Oil, Price, Surge, Big, Issue, WAS...</td>\n",
       "      <td>greenspan oil price surge big issue washington...</td>\n",
       "      <td>[[-1.4591, -0.059439, 1.4654, 1.8356, -0.20327...</td>\n",
       "      <td>[(Greenspan, PROPN, B, PERSON), (Oil, PROPN, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27505</th>\n",
       "      <td>Pay Up for Growth Legg Mason's Mary Chris Gay ...</td>\n",
       "      <td>2</td>\n",
       "      <td>(Pay, Up, for, Growth, Legg, Mason, 's, Mary, ...</td>\n",
       "      <td>23</td>\n",
       "      <td>[Pay, Growth, Legg, Mason, Mary, Chris, Gay, k...</td>\n",
       "      <td>pay growth legg mason mary chris gay know pay ...</td>\n",
       "      <td>[[1.5095, 4.3979, -0.40606, 0.75999, 1.7007, -...</td>\n",
       "      <td>[(Pay, VERB, O, ), (Growth, PROPN, B, ORG), (L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30671</th>\n",
       "      <td>Hubble Space Telescope Spies a Bright Supernov...</td>\n",
       "      <td>3</td>\n",
       "      <td>(Hubble, Space, Telescope, Spies, a, Bright, S...</td>\n",
       "      <td>48</td>\n",
       "      <td>[Hubble, Space, Telescope, Spies, Bright, Supe...</td>\n",
       "      <td>hubble space telescope spies bright supernova ...</td>\n",
       "      <td>[[0.17506, 0.6873, -4.2802, -1.1594, -0.017178...</td>\n",
       "      <td>[(Hubble, PROPN, B, FAC), (Space, PROPN, I, FA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>Dems Challenge Bush in New Campaign Ad (AP) AP...</td>\n",
       "      <td>0</td>\n",
       "      <td>(Dems, Challenge, Bush, in, New, Campaign, Ad,...</td>\n",
       "      <td>52</td>\n",
       "      <td>[Dems, Challenge, Bush, New, Campaign, Ad, AP,...</td>\n",
       "      <td>dems challenge bush new campaign ad ap ap demo...</td>\n",
       "      <td>[[-1.608, 7.8588, -4.869, 5.3426, 2.8102, 2.08...</td>\n",
       "      <td>[(Dems, PROPN, B, ORG), (Challenge, PROPN, I, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>Thousands riot after Pakistan funeral  SIALKOT...</td>\n",
       "      <td>0</td>\n",
       "      <td>(Thousands, riot, after, Pakistan, funeral,  ,...</td>\n",
       "      <td>54</td>\n",
       "      <td>[Thousands, riot, Pakistan, funeral, SIALKOT, ...</td>\n",
       "      <td>thousand riot pakistan funeral sialkot pakista...</td>\n",
       "      <td>[[-1.4431, -1.7198, -0.84067, 2.7954, 0.33977,...</td>\n",
       "      <td>[(Thousands, NOUN, B, CARDINAL), (riot, NOUN, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label  \\\n",
       "9305   Greenspan: Oil Price Surge Not Big Issue WASHI...      0   \n",
       "27505  Pay Up for Growth Legg Mason's Mary Chris Gay ...      2   \n",
       "30671  Hubble Space Telescope Spies a Bright Supernov...      3   \n",
       "7612   Dems Challenge Bush in New Campaign Ad (AP) AP...      0   \n",
       "648    Thousands riot after Pakistan funeral  SIALKOT...      0   \n",
       "\n",
       "                                                  tokens  tokens_count  \\\n",
       "9305   (Greenspan, :, Oil, Price, Surge, Not, Big, Is...            82   \n",
       "27505  (Pay, Up, for, Growth, Legg, Mason, 's, Mary, ...            23   \n",
       "30671  (Hubble, Space, Telescope, Spies, a, Bright, S...            48   \n",
       "7612   (Dems, Challenge, Bush, in, New, Campaign, Ad,...            52   \n",
       "648    (Thousands, riot, after, Pakistan, funeral,  ,...            54   \n",
       "\n",
       "                                         tokens_filtered  \\\n",
       "9305   [Greenspan, Oil, Price, Surge, Big, Issue, WAS...   \n",
       "27505  [Pay, Growth, Legg, Mason, Mary, Chris, Gay, k...   \n",
       "30671  [Hubble, Space, Telescope, Spies, Bright, Supe...   \n",
       "7612   [Dems, Challenge, Bush, New, Campaign, Ad, AP,...   \n",
       "648    [Thousands, riot, Pakistan, funeral, SIALKOT, ...   \n",
       "\n",
       "                                           text_filtered  \\\n",
       "9305   greenspan oil price surge big issue washington...   \n",
       "27505  pay growth legg mason mary chris gay know pay ...   \n",
       "30671  hubble space telescope spies bright supernova ...   \n",
       "7612   dems challenge bush new campaign ad ap ap demo...   \n",
       "648    thousand riot pakistan funeral sialkot pakista...   \n",
       "\n",
       "                                         text_embeddings  \\\n",
       "9305   [[-1.4591, -0.059439, 1.4654, 1.8356, -0.20327...   \n",
       "27505  [[1.5095, 4.3979, -0.40606, 0.75999, 1.7007, -...   \n",
       "30671  [[0.17506, 0.6873, -4.2802, -1.1594, -0.017178...   \n",
       "7612   [[-1.608, 7.8588, -4.869, 5.3426, 2.8102, 2.08...   \n",
       "648    [[-1.4431, -1.7198, -0.84067, 2.7954, 0.33977,...   \n",
       "\n",
       "                                                text_ner  \n",
       "9305   [(Greenspan, PROPN, B, PERSON), (Oil, PROPN, O...  \n",
       "27505  [(Pay, VERB, O, ), (Growth, PROPN, B, ORG), (L...  \n",
       "30671  [(Hubble, PROPN, B, FAC), (Space, PROPN, I, FA...  \n",
       "7612   [(Dems, PROPN, B, ORG), (Challenge, PROPN, I, ...  \n",
       "648    [(Thousands, NOUN, B, CARDINAL), (riot, NOUN, ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data['tokens_filtered'] = small_data['tokens'].apply(filter_tokens)\n",
    "small_data['text_filtered'] = small_data['tokens_filtered'].apply(filter_text)\n",
    "small_data['text_embeddings'] = small_data['tokens_filtered'].apply(text_embeddings)\n",
    "small_data['text_ner'] = small_data['tokens_filtered'].apply(text_ner)\n",
    "small_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_entities(text_filtered):\n",
    "    entity_dict = {}\n",
    "    doc = nlp(text_filtered)\n",
    "    for ent in doc.ents:\n",
    "        if str(ent) not in entity_dict:\n",
    "            entity_dict[ent.lemma_] = (ent.root.pos_, ent.label_)\n",
    "\n",
    "    # non_entity_strings = [token for token in doc \n",
    "    #                     if token.text not in entity_dict \n",
    "    #                     and token.ent_iob_ == \"O\"\n",
    "    #                     and token.pos_ != 'SPACE']\n",
    "    # entity_dict.update({token.lemma_: (token.pos_, None) for token in non_entity_strings})\n",
    "\n",
    "    return entity_dict\n",
    "\n",
    "small_data['entity_dict'] = small_data['text_filtered'].apply(process_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_count</th>\n",
       "      <th>tokens_filtered</th>\n",
       "      <th>text_filtered</th>\n",
       "      <th>text_embeddings</th>\n",
       "      <th>text_ner</th>\n",
       "      <th>entity_dict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30680</th>\n",
       "      <td>Cisco warns of flaws in ACS product BOSTON - N...</td>\n",
       "      <td>3</td>\n",
       "      <td>(Cisco, warns, of, flaws, in, ACS, product, BO...</td>\n",
       "      <td>39</td>\n",
       "      <td>[Cisco, warns, flaws, ACS, product, BOSTON, Ne...</td>\n",
       "      <td>cisco warn flaw acs product boston networking ...</td>\n",
       "      <td>[[-1.9552, 0.29589, 4.3212, 1.4282, -0.63587, ...</td>\n",
       "      <td>[(Cisco, PROPN, B, ORG), (warns, VERB, O, ), (...</td>\n",
       "      <td>{'cisco': ('PROPN', 'GPE'), 'flaw acs product'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19237</th>\n",
       "      <td>Atlanta Braves, Smoltz Agree to New Deal  ATLA...</td>\n",
       "      <td>1</td>\n",
       "      <td>(Atlanta, Braves, ,, Smoltz, Agree, to, New, D...</td>\n",
       "      <td>62</td>\n",
       "      <td>[Atlanta, Braves, Smoltz, Agree, New, Deal, AT...</td>\n",
       "      <td>atlanta braves smoltz agree new deal atlanta s...</td>\n",
       "      <td>[[-1.9546, 2.2159, -0.65238, 2.7719, 3.6104, 1...</td>\n",
       "      <td>[(Atlanta, PROPN, B, ORG), (Braves, PROPN, I, ...</td>\n",
       "      <td>{'atlanta': ('PROPN', 'GPE'), 'atlanta sports ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21550</th>\n",
       "      <td>Oracle plots course for growth Oracle #39;s ho...</td>\n",
       "      <td>2</td>\n",
       "      <td>(Oracle, plots, course, for, growth, Oracle, #...</td>\n",
       "      <td>45</td>\n",
       "      <td>[Oracle, plots, course, growth, Oracle, 39;s, ...</td>\n",
       "      <td>oracle plot course growth oracle 39;s hostile ...</td>\n",
       "      <td>[[0.20333, -2.2937, 4.0433, 1.9336, 1.5848, 0....</td>\n",
       "      <td>[(Oracle, NOUN, B, ORG), (plots, NOUN, O, ), (...</td>\n",
       "      <td>{'oracle': ('PROPN', 'ORG'), 'oracle 39;s': ('...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Telstra for sale at \\$5 a share TREASURER Pete...</td>\n",
       "      <td>0</td>\n",
       "      <td>(Telstra, for, sale, at, \\$5, a, share, TREASU...</td>\n",
       "      <td>37</td>\n",
       "      <td>[Telstra, sale, \\$5, share, TREASURER, Peter, ...</td>\n",
       "      <td>telstra sale \\$5 share treasurer peter costell...</td>\n",
       "      <td>[[-0.32288, 4.5599, -0.17286, -1.8951, 2.1669,...</td>\n",
       "      <td>[(Telstra, PROPN, B, ORG), (sale, NOUN, O, ), ...</td>\n",
       "      <td>{'telstra': ('PROPN', 'NORP'), '\\$5': ('PROPN'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>Kashmir solution still far off despite Musharr...</td>\n",
       "      <td>0</td>\n",
       "      <td>(Kashmir, solution, still, far, off, despite, ...</td>\n",
       "      <td>54</td>\n",
       "      <td>[Kashmir, solution, far, despite, Musharraf, g...</td>\n",
       "      <td>kashmir solution far despite musharraf gambit ...</td>\n",
       "      <td>[[0.20171, 1.6834, -2.7667, 3.8716, 5.7647, 0....</td>\n",
       "      <td>[(Kashmir, PROPN, B, LOC), (solution, NOUN, O,...</td>\n",
       "      <td>{'kashmir': ('PROPN', 'LOC'), 'afp afp': ('PRO...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label  \\\n",
       "30680  Cisco warns of flaws in ACS product BOSTON - N...      3   \n",
       "19237  Atlanta Braves, Smoltz Agree to New Deal  ATLA...      1   \n",
       "21550  Oracle plots course for growth Oracle #39;s ho...      2   \n",
       "47     Telstra for sale at \\$5 a share TREASURER Pete...      0   \n",
       "555    Kashmir solution still far off despite Musharr...      0   \n",
       "\n",
       "                                                  tokens  tokens_count  \\\n",
       "30680  (Cisco, warns, of, flaws, in, ACS, product, BO...            39   \n",
       "19237  (Atlanta, Braves, ,, Smoltz, Agree, to, New, D...            62   \n",
       "21550  (Oracle, plots, course, for, growth, Oracle, #...            45   \n",
       "47     (Telstra, for, sale, at, \\$5, a, share, TREASU...            37   \n",
       "555    (Kashmir, solution, still, far, off, despite, ...            54   \n",
       "\n",
       "                                         tokens_filtered  \\\n",
       "30680  [Cisco, warns, flaws, ACS, product, BOSTON, Ne...   \n",
       "19237  [Atlanta, Braves, Smoltz, Agree, New, Deal, AT...   \n",
       "21550  [Oracle, plots, course, growth, Oracle, 39;s, ...   \n",
       "47     [Telstra, sale, \\$5, share, TREASURER, Peter, ...   \n",
       "555    [Kashmir, solution, far, despite, Musharraf, g...   \n",
       "\n",
       "                                           text_filtered  \\\n",
       "30680  cisco warn flaw acs product boston networking ...   \n",
       "19237  atlanta braves smoltz agree new deal atlanta s...   \n",
       "21550  oracle plot course growth oracle 39;s hostile ...   \n",
       "47     telstra sale \\$5 share treasurer peter costell...   \n",
       "555    kashmir solution far despite musharraf gambit ...   \n",
       "\n",
       "                                         text_embeddings  \\\n",
       "30680  [[-1.9552, 0.29589, 4.3212, 1.4282, -0.63587, ...   \n",
       "19237  [[-1.9546, 2.2159, -0.65238, 2.7719, 3.6104, 1...   \n",
       "21550  [[0.20333, -2.2937, 4.0433, 1.9336, 1.5848, 0....   \n",
       "47     [[-0.32288, 4.5599, -0.17286, -1.8951, 2.1669,...   \n",
       "555    [[0.20171, 1.6834, -2.7667, 3.8716, 5.7647, 0....   \n",
       "\n",
       "                                                text_ner  \\\n",
       "30680  [(Cisco, PROPN, B, ORG), (warns, VERB, O, ), (...   \n",
       "19237  [(Atlanta, PROPN, B, ORG), (Braves, PROPN, I, ...   \n",
       "21550  [(Oracle, NOUN, B, ORG), (plots, NOUN, O, ), (...   \n",
       "47     [(Telstra, PROPN, B, ORG), (sale, NOUN, O, ), ...   \n",
       "555    [(Kashmir, PROPN, B, LOC), (solution, NOUN, O,...   \n",
       "\n",
       "                                             entity_dict  \n",
       "30680  {'cisco': ('PROPN', 'GPE'), 'flaw acs product'...  \n",
       "19237  {'atlanta': ('PROPN', 'GPE'), 'atlanta sports ...  \n",
       "21550  {'oracle': ('PROPN', 'ORG'), 'oracle 39;s': ('...  \n",
       "47     {'telstra': ('PROPN', 'NORP'), '\\$5': ('PROPN'...  \n",
       "555    {'kashmir': ('PROPN', 'LOC'), 'afp afp': ('PRO...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the filtered text and tokens in its own dataframe and save to csv\n",
    "#small_data_filtered = small_data[['text_filtered', 'label',]]\n",
    "#small_data_filtered.to_csv('datasets/small_data_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# split the data into training and testing sets\n",
    "train, test = train_test_split(small_data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_features = []\n",
    "train_features_embeddes = []\n",
    "train_features_text_and_tokens = []\n",
    "train_features_entities = []\n",
    "train_features_no_er = []\n",
    "train_labels = []\n",
    "test_features = []\n",
    "test_features_embeddes = []\n",
    "test_features_text_and_tokens = []\n",
    "test_features_entities = []\n",
    "test_features_no_er = []\n",
    "test_labels = []\n",
    "\n",
    "for index, row in train.iterrows():\n",
    "    entity_dict = row['entity_dict']\n",
    "    word_embeddings = np.array(row['text_embeddings'])  # Convert list to numpy array\n",
    "    word_embeddings_doc = np.mean(word_embeddings, axis=0)  # Average the word embeddings\n",
    "    text_filtered = row['text_filtered']\n",
    "    tokens_filtered = row['tokens_filtered']\n",
    "    label = row['label']\n",
    "    \n",
    "    # All features\n",
    "    features = {\n",
    "        'entity_dict': str(entity_dict),  # Convert dictionary to string\n",
    "        'word_embeddings': [str(embedding) for embedding in word_embeddings.tolist()],  # Convert numpy array to list of strings\n",
    "        'text_filtered': str(text_filtered),  # Convert token to string\n",
    "        'tokens_filtered': [str(token) for token in tokens_filtered]  # Convert tokens to strings\n",
    "    }\n",
    "    train_features.append(features)\n",
    "    \n",
    "    # Only embeddings\n",
    "    features_embedds = {\n",
    "        'word_embeddings': [str(embedding) for embedding in word_embeddings.tolist()],  # Convert numpy array to list of strings\n",
    "    } \n",
    "    train_features_embeddes.append(features_embedds)\n",
    "    \n",
    "    # Only text and tokens\n",
    "    features_text_and_tokens = {\n",
    "        'text_filtered': str(text_filtered),  # Convert token to string\n",
    "        'tokens_filtered': [str(token) for token in tokens_filtered]  # Convert tokens to strings\n",
    "    }\n",
    "    train_features_text_and_tokens.append(features_text_and_tokens)\n",
    "    \n",
    "    # Only entities\n",
    "    features_entities = {\n",
    "        'entity_dict': str(entity_dict),  # Convert dictionary to string\n",
    "    }\n",
    "    train_features_entities.append(features_entities)\n",
    "    \n",
    "    # No NER\n",
    "    features_no_ner = {\n",
    "        'word_embeddings': [str(embedding) for embedding in word_embeddings.tolist()],  # Convert numpy array to list of strings\n",
    "        'text_filtered': str(text_filtered),  # Convert token to string\n",
    "        'tokens_filtered': [str(token) for token in tokens_filtered]  # Convert tokens to strings\n",
    "    } \n",
    "    train_features_no_er.append(features_no_ner)\n",
    "    \n",
    "    train_labels.append(label)\n",
    "    \n",
    "for index, row in test.iterrows():\n",
    "    entity_dict = row['entity_dict']\n",
    "    word_embeddings = np.array(row['text_embeddings'])  # Convert list to numpy array\n",
    "    word_embeddings_doc = np.mean(word_embeddings, axis=0)  # Average the word embeddings\n",
    "    text_filtered = row['text_filtered']\n",
    "    tokens_filtered = row['tokens_filtered']\n",
    "    label = row['label']\n",
    "    \n",
    "    features_test = {\n",
    "        'entity_dict': str(entity_dict),  # Convert dictionary to string\n",
    "        'word_embeddings': [str(embedding) for embedding in word_embeddings.tolist()],  # Convert numpy array to list of strings\n",
    "        'text_filtered': str(text_filtered),  # Convert token to string\n",
    "        'tokens_filtered': [str(token) for token in tokens_filtered]  # Convert tokens to strings\n",
    "    }\n",
    "    test_features.append(features_test)\n",
    "    \n",
    "    # Only embeddings\n",
    "    features_embedds = {\n",
    "        'word_embeddings': [str(embedding) for embedding in word_embeddings.tolist()],  # Convert numpy array to list of strings\n",
    "    } \n",
    "    test_features_embeddes.append(features_embedds)\n",
    "    \n",
    "    # Only text and tokens\n",
    "    features_text_and_tokens = {\n",
    "        'text_filtered': str(text_filtered),  # Convert token to string\n",
    "        'tokens_filtered': [str(token) for token in tokens_filtered]  # Convert tokens to strings\n",
    "    }\n",
    "    test_features_text_and_tokens.append(features_text_and_tokens)\n",
    "    \n",
    "    # Only entities\n",
    "    features_entities = {\n",
    "        'entity_dict': str(entity_dict),  # Convert dictionary to string\n",
    "    }\n",
    "    test_features_entities.append(features_entities)\n",
    "    \n",
    "    # No NER\n",
    "    features_no_ner = {\n",
    "        'word_embeddings': [str(embedding) for embedding in word_embeddings.tolist()],  # Convert numpy array to list of strings\n",
    "        'text_filtered': str(text_filtered),  # Convert token to string\n",
    "        'tokens_filtered': [str(token) for token in tokens_filtered]  # Convert tokens to strings\n",
    "    } \n",
    "    test_features_no_er.append(features_no_ner)\n",
    "    test_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def modelFit(model, param_grid, train_features, test_features):\n",
    "    pipeline = Pipeline([\n",
    "        ('vectorizer', DictVectorizer()),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "\n",
    "    # Create the GridSearchCV object\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=10, scoring='accuracy')\n",
    "\n",
    "    # Fit the GridSearchCV object\n",
    "    grid_search.fit(train_features, train_labels)\n",
    "\n",
    "    # Get the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Predict the test data\n",
    "    preds = best_model.predict(test_features)\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    accuracy = accuracy_score(test_labels, preds)\n",
    "    f1 = f1_score(test_labels, preds, average='weighted')\n",
    "    precision = precision_score(test_labels, preds, average='weighted')\n",
    "    recall = recall_score(test_labels, preds, average='weighted')\n",
    "\n",
    "    results = {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'confusion_matrix': confusion_matrix(test_labels, preds),\n",
    "        'best_params': grid_search.best_params_\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## NB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.904625, 'f1': 0.9044251932117491, 'precision': 0.9045102173212911, 'recall': 0.904625, 'confusion_matrix': array([[1817,   67,  111,   52],\n",
      "       [  21, 1924,   12,   10],\n",
      "       [  59,   17, 1740,  170],\n",
      "       [  77,   21,  146, 1756]]), 'best_params': {}}\n"
     ]
    }
   ],
   "source": [
    "results = modelFit(MultinomialNB(), {}, train_features, test_features)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only text and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.90625, 'f1': 0.9060497362386052, 'precision': 0.9060131772958145, 'recall': 0.90625, 'confusion_matrix': array([[1825,   65,  109,   48],\n",
      "       [  24, 1923,    9,   11],\n",
      "       [  62,   13, 1745,  166],\n",
      "       [  87,   18,  138, 1757]]), 'best_params': {}}\n"
     ]
    }
   ],
   "source": [
    "results = modelFit(MultinomialNB(), {}, train_features_text_and_tokens, test_features_text_and_tokens)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only entities and PoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.301625, 'f1': 0.20236541079088105, 'precision': 0.5700431432380205, 'recall': 0.301625, 'confusion_matrix': array([[  33, 1972,   10,   32],\n",
      "       [   1, 1931,    8,   27],\n",
      "       [   4, 1807,   79,   96],\n",
      "       [   7, 1581,   42,  370]]), 'best_params': {}}\n"
     ]
    }
   ],
   "source": [
    "results = modelFit(MultinomialNB(), {}, train_features_entities, test_features_entities)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Dense Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.900375, 'f1': 0.9001754048431471, 'precision': 0.9002720823557113, 'recall': 0.900375, 'confusion_matrix': array([[1814,   68,  111,   54],\n",
      "       [  20, 1922,   12,   13],\n",
      "       [  64,   20, 1721,  181],\n",
      "       [  73,   21,  160, 1746]]), 'best_params': {}}\n"
     ]
    }
   ],
   "source": [
    "results = modelFit(MultinomialNB(), {}, train_features_embeddes, test_features_embeddes)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.903875, 'f1': 0.9036569269056173, 'precision': 0.9037067168247837, 'recall': 0.903875, 'confusion_matrix': array([[1816,   68,  111,   52],\n",
      "       [  21, 1924,   12,   10],\n",
      "       [  61,   17, 1737,  171],\n",
      "       [  80,   21,  145, 1754]]), 'best_params': {}}\n"
     ]
    }
   ],
   "source": [
    "results = modelFit(MultinomialNB(), {}, train_features_no_er, test_features_no_er)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gdrp7\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\gdrp7\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\gdrp7\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\gdrp7\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\gdrp7\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\gdrp7\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "results = modelFit(LogisticRegression(), {\n",
    "    'classifier__C': [0.1, 1.0, 10.0],\n",
    "    'classifier__penalty': ['l1', 'l2'],\n",
    "    'classifier__solver': ['liblinear', 'saga']\n",
    "}, train_features, test_features)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only text and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(LogisticRegression(), {\n",
    "    'classifier__C': [0.1, 1.0, 10.0],\n",
    "    'classifier__penalty': ['l1', 'l2'],\n",
    "    'classifier__solver': ['liblinear', 'saga']\n",
    "}, train_features_text_and_tokens, test_features_text_and_tokens)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only entities and PoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(LogisticRegression(), {\n",
    "    'classifier__C': [0.1, 1.0, 10.0],\n",
    "    'classifier__penalty': ['l1', 'l2'],\n",
    "    'classifier__solver': ['liblinear', 'saga']\n",
    "}, train_features_entities, test_features_entities)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only dense embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(LogisticRegression(), {\n",
    "    'classifier__C': [0.1, 1.0, 10.0],\n",
    "    'classifier__penalty': ['l1', 'l2'],\n",
    "    'classifier__solver': ['liblinear', 'saga']\n",
    "}, train_features_embeddes, test_features_embeddes)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(LogisticRegression(), {\n",
    "    'classifier__C': [0.1, 1.0, 10.0],\n",
    "    'classifier__penalty': ['l1', 'l2'],\n",
    "    'classifier__solver': ['liblinear', 'saga']\n",
    "}, train_features_no_er, test_features_no_er)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## DTC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "results = modelFit(DecisionTreeClassifier(), {\n",
    "        'classifier__max_depth': [None, 5, 10, 15],\n",
    "        'classifier__min_samples_split': [2, 5, 10]\n",
    "    }, train_features, test_features)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only text and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(DecisionTreeClassifier(), {\n",
    "        'classifier__max_depth': [None, 5, 10, 15],\n",
    "        'classifier__min_samples_split': [2, 5, 10]\n",
    "    }, train_features_text_and_tokens, test_features_text_and_tokens)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only entites and PoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(DecisionTreeClassifier(), {\n",
    "        'classifier__max_depth': [None, 5, 10, 15],\n",
    "        'classifier__min_samples_split': [2, 5, 10]\n",
    "    }, train_features_entities, test_features_entities)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only dense embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(DecisionTreeClassifier(), {\n",
    "        'classifier__max_depth': [None, 5, 10, 15],\n",
    "        'classifier__min_samples_split': [2, 5, 10]\n",
    "    }, train_features_embeddes, test_features_embeddes)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(DecisionTreeClassifier(), {\n",
    "        'classifier__max_depth': [None, 5, 10, 15],\n",
    "        'classifier__min_samples_split': [2, 5, 10]\n",
    "    }, train_features_no_er, test_features_no_er)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklean.ensemble import RandomForestClassifier\n",
    "\n",
    "results = modelFit(RandomForestClassifier(), {\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__max_depth': [None, 5, 10, 15],\n",
    "        'classifier__min_samples_split': [5, 10, 20, 30]\n",
    "    }, train_features, test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only text and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(RandomForestClassifier(), {\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__max_depth': [None, 5, 10, 15],\n",
    "        'classifier__min_samples_split': [5, 10, 20, 30]\n",
    "    }, train_features_text_and_tokens, test_features_text_and_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only entities and PoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(RandomForestClassifier(), {\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__max_depth': [None, 5, 10, 15],\n",
    "        'classifier__min_samples_split': [5, 10, 20, 30]\n",
    "    }, train_features_entities, test_features_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only dense embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(RandomForestClassifier(), {\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__max_depth': [None, 5, 10, 15],\n",
    "        'classifier__min_samples_split': [5, 10, 20, 30]\n",
    "    }, train_features_embeddes, test_features_embeddes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(RandomForestClassifier(), {\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__max_depth': [None, 5, 10, 15],\n",
    "        'classifier__min_samples_split': [5, 10, 20, 30]\n",
    "    }, train_features_no_er, test_features_no_er)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "results = modelFit(SVC(), {\n",
    "        'classifier__C': [0.1, 1.0, 10.0],\n",
    "        'classifier__kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "    }, train_features, test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only text and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(SVC(), {\n",
    "        'classifier__C': [0.1, 1.0, 10.0],\n",
    "        'classifier__kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "    }, train_features_text_and_tokens, test_features_text_and_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only entities and PoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(SVC(), {\n",
    "        'classifier__C': [0.1, 1.0, 10.0],\n",
    "        'classifier__kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "    }, train_features_entities, test_features_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only dense embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(SVC(), {\n",
    "        'classifier__C': [0.1, 1.0, 10.0],\n",
    "        'classifier__kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "    }, train_features_embeddes, test_features_embeddes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(SVC(), {\n",
    "        'classifier__C': [0.1, 1.0, 10.0],\n",
    "        'classifier__kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "    }, train_features_no_er, test_features_no_er)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "results = modelFit(XGBClassifier(), {\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__max_depth': [3, 5, 7, 9]\n",
    "    }, train_features, test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only text and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(XGBClassifier(), {\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__max_depth': [3, 5, 7, 9]\n",
    "    }, train_features_text_and_tokens, test_features_text_and_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only entities and PoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(XGBClassifier(), {\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__max_depth': [3, 5, 7, 9]\n",
    "    }, train_features_entities, test_features_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only dense embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(XGBClassifier(), {\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__max_depth': [3, 5, 7, 9]\n",
    "    }, train_features_embeddes, test_features_embeddes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(XGBClassifier(), {\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__max_depth': [3, 5, 7, 9]\n",
    "    }, train_features_no_er, test_features_no_er)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "results = modelFit(GradientBoostingClassifier(), {\n",
    "    'classifier__n_estimators': [50, 100, 200],\n",
    "    'classifier__learning_rate': [0.01, 0.1, 0.5],\n",
    "    'classifier__max_depth': [3, 5, 7]\n",
    "}, train_features, test_features)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only text and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(GradientBoostingClassifier(), {\n",
    "    'classifier__n_estimators': [50, 100, 200],\n",
    "    'classifier__learning_rate': [0.01, 0.1, 0.5],\n",
    "    'classifier__max_depth': [3, 5, 7]\n",
    "}, train_features_text_and_tokens, test_features_text_and_tokens)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only entities and PoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(GradientBoostingClassifier(), {\n",
    "    'classifier__n_estimators': [50, 100, 200],\n",
    "    'classifier__learning_rate': [0.01, 0.1, 0.5],\n",
    "    'classifier__max_depth': [3, 5, 7]\n",
    "}, train_features_entities, test_features_entities)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Dense Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(GradientBoostingClassifier(), {\n",
    "    'classifier__n_estimators': [50, 100, 200],\n",
    "    'classifier__learning_rate': [0.01, 0.1, 0.5],\n",
    "    'classifier__max_depth': [3, 5, 7]\n",
    "}, train_features_embeddes, test_features_embeddes)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(GradientBoostingClassifier(), {\n",
    "    'classifier__n_estimators': [50, 100, 200],\n",
    "    'classifier__learning_rate': [0.01, 0.1, 0.5],\n",
    "    'classifier__max_depth': [3, 5, 7]\n",
    "}, train_features_no_er, test_features_no_er)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "results = modelFit(KNeighborsClassifier(), {\n",
    "    'classifier__n_neighbors': [3, 5, 7, 10],\n",
    "    'classifier__weights': ['uniform', 'distance'],\n",
    "    'classifier__p': [1, 2]  # 1 for Manhattan distance, 2 for Euclidean distance\n",
    "}, train_features, test_features)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only text and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(KNeighborsClassifier(), {\n",
    "    'classifier__n_neighbors': [3, 5, 7, 10],\n",
    "    'classifier__weights': ['uniform', 'distance'],\n",
    "    'classifier__p': [1, 2]  # 1 for Manhattan distance, 2 for Euclidean distance\n",
    "}, train_features_text_and_tokens, test_features_text_and_tokens)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only entities and PoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(KNeighborsClassifier(), {\n",
    "    'classifier__n_neighbors': [3, 5, 7, 10],\n",
    "    'classifier__weights': ['uniform', 'distance'],\n",
    "    'classifier__p': [1, 2]  # 1 for Manhattan distance, 2 for Euclidean distance\n",
    "}, train_features_entities, test_features_entities)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Dense Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(KNeighborsClassifier(), {\n",
    "    'classifier__n_neighbors': [3, 5, 7, 10],\n",
    "    'classifier__weights': ['uniform', 'distance'],\n",
    "    'classifier__p': [1, 2]  # 1 for Manhattan distance, 2 for Euclidean distance\n",
    "}, train_features_embeddes, test_features_embeddes)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(KNeighborsClassifier(), {\n",
    "    'classifier__n_neighbors': [3, 5, 7, 10],\n",
    "    'classifier__weights': ['uniform', 'distance'],\n",
    "    'classifier__p': [1, 2]  # 1 for Manhattan distance, 2 for Euclidean distance\n",
    "}, train_features_no_er, test_features_no_er)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "results = modelFit(MLPClassifier(), {\n",
    "    'classifier__hidden_layer_sizes': [(100,), (50, 50), (50, 100, 50)],\n",
    "    'classifier__activation': ['logistic', 'relu'],\n",
    "    'classifier__solver': ['adam', 'sgd'],\n",
    "    'classifier__alpha': [0.0001, 0.001, 0.01],\n",
    "    'classifier__learning_rate': ['constant', 'adaptive']\n",
    "}, train_features, test_features)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only text and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(MLPClassifier(), {\n",
    "    'classifier__hidden_layer_sizes': [(100,), (50, 50), (50, 100, 50)],\n",
    "    'classifier__activation': ['logistic', 'relu'],\n",
    "    'classifier__solver': ['adam', 'sgd'],\n",
    "    'classifier__alpha': [0.0001, 0.001, 0.01],\n",
    "    'classifier__learning_rate': ['constant', 'adaptive']\n",
    "}, train_features_text_and_tokens, test_features_text_and_tokens)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only entities and PoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(MLPClassifier(), {\n",
    "    'classifier__hidden_layer_sizes': [(100,), (50, 50), (50, 100, 50)],\n",
    "    'classifier__activation': ['logistic', 'relu'],\n",
    "    'classifier__solver': ['adam', 'sgd'],\n",
    "    'classifier__alpha': [0.0001, 0.001, 0.01],\n",
    "    'classifier__learning_rate': ['constant', 'adaptive']\n",
    "}, train_features_entities, test_features_entities)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Dense Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(MLPClassifier(), {\n",
    "    'classifier__hidden_layer_sizes': [(100,), (50, 50), (50, 100, 50)],\n",
    "    'classifier__activation': ['logistic', 'relu'],\n",
    "    'classifier__solver': ['adam', 'sgd'],\n",
    "    'classifier__alpha': [0.0001, 0.001, 0.01],\n",
    "    'classifier__learning_rate': ['constant', 'adaptive']\n",
    "}, train_features_embeddes, test_features_embeddes)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelFit(MLPClassifier(), {\n",
    "    'classifier__hidden_layer_sizes': [(100,), (50, 50), (50, 100, 50)],\n",
    "    'classifier__activation': ['logistic', 'relu'],\n",
    "    'classifier__solver': ['adam', 'sgd'],\n",
    "    'classifier__alpha': [0.0001, 0.001, 0.01],\n",
    "    'classifier__learning_rate': ['constant', 'adaptive']\n",
    "}, train_features_no_er, test_features_no_er)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
